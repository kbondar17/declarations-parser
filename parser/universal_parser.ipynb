{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from typing import Union\n",
    "import csv\n",
    "import statistics\n",
    "\n",
    "import typing\n",
    "import io\n",
    "import numpy as np\n",
    "from docx import Document\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "%config Completer.use_jedi = False\n",
    "from pathlib import Path\n",
    "# import tqdm.notebook.tqdm as tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.basicConfig(format=u'%(filename)+13s [ LINE:%(lineno)-4s] %(levelname)-8s %(message)s',\n",
    "                    level=logging.DEBUG)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import camelot\n",
    "# tables = camelot.read_pdf(file, line_tol=2, joint_tol=10, line_scale=40, copy_text=['v'], pages='1-end') # , flavor='stream' row_tol=10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#split_text , strip_text  line_tol=2, joint_tol=2, line_scale=15 \n",
    "\n",
    "# tables[8].df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# КОРОЧЕ ЮЗАЕМ КАМЕЛОТ (мб предлагать юзеру настройки распознавания)\n",
    "\n",
    "# res = []\n",
    "# for e in tables:\n",
    "#     res.append(e.df) \n",
    "\n",
    "# df = pd.concat(res)\n",
    "# df.to_excel('camelot_test.xlsx')\n",
    "\n",
    "\n",
    "# len(tables)\n",
    "# tables[1].df\n",
    "# df = tables[2].df \n",
    "# camelot.plot(tables[0], kind='joint')\n",
    "# df\n",
    "\n",
    "#tables[0].parsing_report\n",
    "\n",
    "#tables[0].to_csv('foo.csv') # to_json, to_excel,   down, to_sqlite\n",
    "#tables[0].df # get a pandas DataFrame!\n",
    "\n",
    "class PdfParser:\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_pdf_to_df(filename) -> list[pd.DataFrame]:\n",
    "        tables = camelot.read_pdf(filename, line_tol=2, joint_tol=10, line_scale=40, copy_text=['v'], pages='1-end') # , flavor='stream' row_tol=10\n",
    "        tables = [e.df for e in tables]\n",
    "        return tables\n",
    "\n",
    "\n",
    "\n",
    "# base = 'data_idus/pdf/converted/'\n",
    "# file = '189273_2020_Rektor,_prorektory,_glavnyi_bukhgalter.pdf'\n",
    "file = \"D:/PROGR/LEARN_PYTHON/Declarator/declarations-parser/data_ids/pdf/converted/90569_2018_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii.pdf\"\n",
    "# par = PdfParser()\n",
    "# res = par.convert_pdf_to_df(file)\n",
    "# res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'100206_2019',\n",
       " '101232_2018',\n",
       " '102031_2019',\n",
       " '102421_2019',\n",
       " '102541_2019',\n",
       " '102585_2019',\n",
       " '102586_2019',\n",
       " '102907_2019',\n",
       " '103026_2019',\n",
       " '104576_2019',\n",
       " '104619_2019',\n",
       " '176299_2020',\n",
       " '177336_2020',\n",
       " '177672_2020',\n",
       " '178290_2020',\n",
       " '178303_2020',\n",
       " '178403_2020',\n",
       " '178414_2020',\n",
       " '178423_2020',\n",
       " '179161_2020',\n",
       " '179252_2020',\n",
       " '179394_2020',\n",
       " '179467_2020',\n",
       " '179512_2020',\n",
       " '179590_2020',\n",
       " '179663_2020',\n",
       " '179911_2020',\n",
       " '180112_2020',\n",
       " '180258_2020',\n",
       " '180263_2020',\n",
       " '180371_2020',\n",
       " '180385_2020',\n",
       " '182639_2020',\n",
       " '183392_2020',\n",
       " '183413_2020',\n",
       " '184287_2018',\n",
       " '184503_2020',\n",
       " '184919_2020',\n",
       " '186491_2020',\n",
       " '186613_2020',\n",
       " '187331_2020',\n",
       " '187741_2020',\n",
       " '187742_2020',\n",
       " '188290_2020',\n",
       " '189123_2020',\n",
       " '189320_2020',\n",
       " '189322_2020',\n",
       " '189496_2020',\n",
       " '189701_2020',\n",
       " '189856_2020',\n",
       " '83292_2016',\n",
       " '83310_2016',\n",
       " '83311_2017',\n",
       " '83312_2018',\n",
       " '83316_2018',\n",
       " '83318_2017',\n",
       " '83321_2018',\n",
       " '83335_2018',\n",
       " '83336_2017',\n",
       " '83338_2018',\n",
       " '83339_2017',\n",
       " '83340_2018',\n",
       " '83341_2017',\n",
       " '83342_2018',\n",
       " '83343_2017',\n",
       " '83344_2016',\n",
       " '83349_2018',\n",
       " '83350_2017',\n",
       " '83360_2018',\n",
       " '83364_2018',\n",
       " '83365_2018',\n",
       " '83376_2018',\n",
       " '83378_2018',\n",
       " '83381_2019',\n",
       " '83382_2018',\n",
       " '83383_2017',\n",
       " '83384_2017',\n",
       " '83735_2018',\n",
       " '83736_2017',\n",
       " '84462_2018',\n",
       " '84463_2017',\n",
       " '84524_2019',\n",
       " '84547_2019',\n",
       " '84548_2018',\n",
       " '84562_2019',\n",
       " '84563_2018',\n",
       " '84753_2019',\n",
       " '84754_2018',\n",
       " '84755_2017',\n",
       " '84860_2018',\n",
       " '84903_2017',\n",
       " '84904_2018',\n",
       " '84925_2019',\n",
       " '85029_2018',\n",
       " '85082_2018',\n",
       " '85083_2017',\n",
       " '85273_2019',\n",
       " '85511_2019',\n",
       " '85512_2018',\n",
       " '85861_2019',\n",
       " '86671_2019',\n",
       " '86741_2019',\n",
       " '86949_2019',\n",
       " '87364_2018',\n",
       " '88260_2018',\n",
       " '88669_2018',\n",
       " '88670_2017',\n",
       " '88775_2018',\n",
       " '88776_2017',\n",
       " '88942_2019',\n",
       " '88943_2018',\n",
       " '88944_2017',\n",
       " '90303_2018',\n",
       " '90629_2017',\n",
       " '90665_2019',\n",
       " '90666_2018',\n",
       " '90871_2018',\n",
       " '92762_2019',\n",
       " '92796_2019',\n",
       " '95518_2019',\n",
       " '95807_2019',\n",
       " '95822_2019',\n",
       " '96034_2019',\n",
       " '96354_2019',\n",
       " '96410_2019',\n",
       " '96461_2019',\n",
       " '96657_2019',\n",
       " '96866_2019',\n",
       " '97332_2019',\n",
       " '97367_2019',\n",
       " '98238_2019',\n",
       " '99715_2019',\n",
       " 'test_rogue.docx',\n",
       " '~$2907_2019',\n",
       " '~$344_2016'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = r'D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\docx'\n",
    "folder_with_ok = r'D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\docx\\cool\\ok2'\n",
    "\n",
    "ne_ok_ids = []\n",
    "ok_ids = []\n",
    "all_ids = []\n",
    "\n",
    "for e in os.listdir(folder):\n",
    "    if e.endswith('docx'):\n",
    "        all_ids.append('_'.join(e.split('_')[:2]))\n",
    "\n",
    "for e in os.listdir(folder_with_ok):\n",
    "    if e.endswith('xlsx'):\n",
    "        ok_ids.append('_'.join(e.split('_')[:2]))\n",
    "\n",
    "# print(len(all_ids))\n",
    "# print(len(ok_ids))\n",
    "\n",
    "\n",
    "\n",
    "all_ids = set(all_ids)\n",
    "ok_ids = set(ok_ids)\n",
    "\n",
    "bad_ids = all_ids - ok_ids\n",
    "bad_ids\n",
    "\n",
    "#for e in os.listdir(folder_with_ok):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_20724/1729719161.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Kirill\\AppData\\Local\\Temp/ipykernel_20724/1729719161.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    def\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class IncorrectHeaders:\n",
    "    \"\"\"класс для таблиц с неопределенными заголовками\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def search_for_office(df: pd.DataFrame):\n",
    "        # ищет названия учреждени внутри таблицы \n",
    "        pass\n",
    "\n",
    "\n",
    "    def parse(self, df: pd.DataFrame) -> typing.Union[pd.DataFrame, False]:\n",
    "        df = self.search_for_office(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "    # если учреждения не нашли, \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CorrectHeadersParser:\n",
    "\n",
    "    '''класс для парсинга таблиц, у которых на месте колонки, которые нам нужны'''\n",
    "\n",
    "    def table_splitter(self, table: pd.DataFrame) -> list[pd.DataFrame]:\n",
    "        '''разделяет таблицы, в которых учреждение указано внутри таблицы'''\n",
    "\n",
    "        def check_if_same(my_array: list) -> bool:\n",
    "            \n",
    "            '''проверяем одинаковые ли колонки'''\n",
    "\n",
    "            if len(set(my_array))>1:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "            # first = my_array[0]\n",
    "            # for e in my_array[1:]:\n",
    "            #     if e != first:\n",
    "            #         return False\n",
    "            # return True\n",
    "\n",
    "        def get_indexes_to_split(table):\n",
    "            index_to_split = []\n",
    "            for e in range(len(table)):\n",
    "                cols = table.iloc[e,:].values\n",
    "                if check_if_same(cols):\n",
    "                    index_to_split.append(e)\n",
    "            return index_to_split\n",
    "\n",
    "\n",
    "        def split_table(table: pd.DataFrame, index_to_split:Union[int, list[int]], file_name) -> list[pd.DataFrame]:\n",
    "            \"\"\"разделяет таблицу в случае когда название учреждения поместили в середину\n",
    "            \n",
    "                -должность-  -имя-  -зарплата-\n",
    "                        -ГБОУ школа 112-\n",
    "                директор     Ваня    100 руб\n",
    "\n",
    "             \"\"\"\n",
    "            dfs = np.array_split(table, index_to_split)\n",
    "            dfs = [e for e in dfs if len(e) > 0]\n",
    "\n",
    "            result_dfs = []\n",
    "            for df in dfs:\n",
    "                office = df.iloc[0,:][0]\n",
    "                df = df.iloc[1:,:] \n",
    "                df['office'] = office\n",
    "                result_dfs.append(df)\n",
    "            \n",
    "            result_dfs = [e for e in result_dfs if not e.empty]\n",
    "            try:\n",
    "                result_dfs = pd.concat(result_dfs)\n",
    "                return result_dfs\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                print('rogue file---', table)\n",
    "                \n",
    "        index_to_split = get_indexes_to_split(table)\n",
    "\n",
    "        if not index_to_split:\n",
    "            return table\n",
    "\n",
    "        splitted_dfs = split_table(table, index_to_split)\n",
    "        return splitted_dfs\n",
    "\n",
    "        \n",
    "    def concat_name(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''соединяем колонки ФИО, если они в разных'''\n",
    "        \n",
    "        if 'name' not in df.columns:\n",
    "            return df\n",
    "        \n",
    "        names_df = df['name']\n",
    "    \n",
    "        if isinstance(names_df, str) or isinstance(names_df, pd.Series):\n",
    "            return df  \n",
    "    \n",
    "        # TODO:\n",
    "        # дропнуть маленькую колонку\n",
    "\n",
    "\n",
    "        names = [' '.join(e) for e in names_df.values]     \n",
    "        \n",
    "        df.drop(columns=['name'], inplace=True)\n",
    "        df['name'] = names\n",
    "        return df\n",
    "\n",
    "\n",
    "    def parse(self, table: pd.DataFrame) -> pd.DataFrame:\n",
    "        table = self.concat_name(table)\n",
    "        table = self.table_splitter(table)\n",
    "        return table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataCleaner:\n",
    "    \"\"\"убирает лишние данные\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_unwanted_symbols(df):        \n",
    "        # TODO: чистка всех колонок\n",
    "        df = df.applymap(lambda x: str(x).replace('\\n', ' '))\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_unwanted_cells(df):\n",
    "        # убирает ячейки с нумерацией\n",
    "        # print('--- DataCleaner.remove_unwanted_cells ---', df.columns)\n",
    "        df = df[~df['position'].astype(str).str.isdigit()]\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_short_rows(df):\n",
    "        # удаляет ряды с недостаточными данными\n",
    "        # ! должно применяться после выбора норм колонок\n",
    "        to_remove = []\n",
    "        for tup in df.itertuples():\n",
    "            res = [len(str(e)) for e in tup]\n",
    "            if statistics.mean(res) < 5:\n",
    "                to_remove.append(tup.Index)\n",
    "        \n",
    "\n",
    "        df.drop(to_remove, inplace=True)\n",
    "        return df\n",
    "             \n",
    "\n",
    "    def clean_df(self, df):\n",
    "        df = self.remove_unwanted_symbols(df)\n",
    "        df = self.remove_unwanted_cells(df)\n",
    "\n",
    "        df = self.remove_short_rows(df)\n",
    "        # print('!!!',df)\n",
    "\n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DocxParser:\n",
    "\n",
    "    def get_docx_tables(self, filename, tab_id=None, **kwargs) -> list[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "            filename:   file name of a Word Document\n",
    "            tab_id:     parse a single table with the index: [tab_id] (counting from 0).\n",
    "                        When [None] - return a list of DataFrames (parse all tables)\n",
    "        \"\"\"\n",
    "        def read_docx_tab(tab, **kwargs):\n",
    "            vf = io.StringIO()\n",
    "            writer = csv.writer(vf)\n",
    "            for row in tab.rows:\n",
    "                writer.writerow(cell.text for cell in row.cells)\n",
    "            vf.seek(0)\n",
    "            return pd.read_csv(vf, **kwargs)\n",
    "\n",
    "        doc = Document(filename)\n",
    "        if tab_id is None:\n",
    "            return [read_docx_tab(tab, **kwargs) for tab in doc.tables]\n",
    "        else:\n",
    "            try:\n",
    "                return read_docx_tab(doc.tables[tab_id], **kwargs)\n",
    "            except IndexError:\n",
    "                print('Error: specified [tab_id]: {}  does not exist.'.format(tab_id))\n",
    "                raise\n",
    "            \n",
    "\n",
    "    def convert_docx_to_df(self, filename: str) -> pd.DataFrame:\n",
    "        assert filename.endswith('docx'), 'Формат должен быть .docx!'\n",
    "            \n",
    "        doc = Document(filename)\n",
    "        # TODO: тут взять текст, который потом прикрутить к\n",
    "\n",
    "        doc_tables = self.get_docx_tables(filename) \n",
    "        \n",
    "        return doc_tables\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cols_we_need = ['name','salary', 'position', 'department']\n",
    "        self.all_docs: list[str]\n",
    "\n",
    "        self.docx_parser = DocxParser()\n",
    "        self.pdf_parser = PdfParser()\n",
    "\n",
    "        self.parse_correct_headers = CorrectHeadersParser()\n",
    "        self.parse_incorrect_headers = ''\n",
    "\n",
    "        self.data_cleaner = DataCleaner()\n",
    "\n",
    "    @staticmethod\n",
    "    def rename_col(col: str) -> str:\n",
    "\n",
    "        print('col before rename cols --', col)\n",
    "        col = col.lower()\n",
    "        if re.search(pattern='(фамилия|имя|фио|ф\\.и\\.о\\.|ф\\.и\\.о|отчество)', string=col):\n",
    "            return \"name\"\n",
    "\n",
    "        elif re.search(pattern='(cреднемесячная|зарпл.|плат[ы, а]|заработн[ой, ая] плат[а, ы]|cреднемесячн[ая, ой]|зарплат[а, ной, ы])', string=col):\n",
    "            return \"salary\"\n",
    "\n",
    "        elif re.search(pattern='(должност[ь, и, ей])', string=col): \n",
    "\n",
    "            return 'position'\n",
    "\n",
    "        elif re.search(pattern='(предприяти[е,я]|учреждени[е,я]|юридическое лицо)', string=col):\n",
    "            return 'department'\n",
    "\n",
    "        return col\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def check_if_columns_ok(cols: tuple) -> bool:\n",
    "        '''проверяем, есть ли в заголовках таблицы название предприятия и другая инфа'''\n",
    "        \n",
    "        cols = list(map(str, cols))\n",
    "        cols = list(map(str.lower, cols))\n",
    "        print('зашли в проверку колонок ---', cols)\n",
    "        ok_cols = 0\n",
    "        company_found = False\n",
    "        for col in cols:\n",
    "            company_pattern = '(предприяти[е,я]|учреждени[е,я]|юридическ[ое,ие])'\n",
    "            res = re.search(pattern=company_pattern, string=col)\n",
    "            if res:\n",
    "                company_found = True\n",
    "                continue\n",
    "            \n",
    "            name_salary_position_pattern = '(фамилия|имя|фио|ф\\.и\\.о\\.|ф\\.и\\.о|отчество|плат[ы, а]|заработная|плата|cреднемесячн[ая, ой]|зарплат[а, ной, ы]|должность|)'\n",
    "            \n",
    "            res = re.search(pattern=name_salary_position_pattern, string=col)\n",
    "            if res:\n",
    "                ok_cols+=1\n",
    "\n",
    "        if company_found and ok_cols > 1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def parse_folder(self, file_path, destination_path):\n",
    "        for file in os.listdir(file_path):\n",
    "            try:\n",
    "                df = self.parse_file(file_path)\n",
    "                df.to_excel(destination_path + file_path)\n",
    "            except Exception as ex:\n",
    "                print(file_path)\n",
    "                print(ex)\n",
    "                print('===')\n",
    "\n",
    "\n",
    "    def parse_file(self, file: str):\n",
    "        if file.endswith('.pdf'):\n",
    "            tables = self.pdf_parser.convert_pdf_to_df(file)\n",
    "        \n",
    "        elif file.endswith('docx'):\n",
    "            tables = self.docx_parser.convert_docx_to_df(file)\n",
    "\n",
    "        else:\n",
    "            logger.error('Допустимы расширения: pdf, docx')\n",
    "\n",
    "        parsed_tables = []\n",
    "        for table in tables:\n",
    " \n",
    "            columns_ok = self.check_if_columns_ok(table)\n",
    "            if not columns_ok:\n",
    "            # пометить?\n",
    "            # если учреждения нет - смотрим параграфы. \n",
    "            # пытаемся найти заголовки, если находим - идем дальше, если нет - дропаем и метим как непаршеный\n",
    "\n",
    "                pass\n",
    "\n",
    "            else:                \n",
    "                # если заголовки ок, оставляем только нужные\n",
    "\n",
    "                table.columns = [self.rename_col(col) for col in table.columns]\n",
    "                \n",
    "                \n",
    "                cols_to_leave = [col for col in table.columns if col in self.cols_we_need]\n",
    "                cols_to_leave = set(cols_to_leave)\n",
    "                table = table[cols_to_leave]\n",
    "                # проверяем на наличие вложенных таблиц и фио, разнесенных на несколько стаоблцов\n",
    "                table = self.parse_correct_headers.parse(table)\n",
    "                # убираем лишние ячейки и символы\n",
    "                table = self.data_cleaner.clean_df(table)\n",
    "                parsed_tables.append(table)\n",
    "               # print('!!!!',table)\n",
    "                \n",
    "\n",
    "\n",
    "        if isinstance(parsed_tables, list):\n",
    "            if parsed_tables:\n",
    "                concat_tables = pd.concat(parsed_tables)\n",
    "                return concat_tables\n",
    "    \n",
    "        elif isinstance(parsed_tables, pd.DataFrame):\n",
    "            if not parsed_tables.empty:\n",
    "                return concat_tables\n",
    "        \n",
    "\n",
    "\n",
    "base = 'data_ids/pdf/converted/'\n",
    "file = '189273_2020_Rektor,_prorektory,_glavnyi_bukhgalter.pdf'\n",
    "def convert_pdf_to_df(filename) -> list[pd.DataFrame]:\n",
    "    tables = camelot.read_pdf(file, line_tol=2, joint_tol=10, line_scale=40, copy_text=['v'], pages='1-end') # , flavor='stream' row_tol=10\n",
    "    tables = [e.df for e in tables]\n",
    "    return tables\n",
    "\n",
    "#file = 'data_ids/pdf/converted/189273_2020_Rektor,_prorektory,_glavnyi_bukhgalter.pdf'\n",
    "file = \"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\docx\\83310_2016_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii.docx\"\n",
    "file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\docx\\102907_2019_Rukovoditeli_podvedomstvennykh_uchrezhdenii_(kul'tura).docx\"\n",
    "# convert_pdf_to_df(file)\n",
    "parser = Parser()\n",
    "# res = parser.parse_file(file)\n",
    "# res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for e in os.listdir('../'):\n",
    "#     if 'test' in e and 'xlsx' in e:\n",
    "#         df = pd.read_excel(e)\n",
    "#         res = parser.parse_file(e)\n",
    "\n",
    "def моя_функция():\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ДОСОБИРАТЬ ПАРСЕР В ОДНО ЦЕЛОЕ\n",
    "#ПОФЕЙКАТЬ ПДФ\n",
    "#ПДФ БУДЕТ РАБОТАТЬ ТОЛЬКО В .PY\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "137dc9a7529e050f0d3404627e217fd4120b6229f927507e6c7e6283ad4b3698"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
