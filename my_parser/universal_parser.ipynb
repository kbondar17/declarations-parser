{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from typing import Union\n",
    "import csv\n",
    "import statistics\n",
    "import pdf2docx\n",
    "\n",
    "import pickle\n",
    "\n",
    "import camelot\n",
    "import typing\n",
    "import io\n",
    "import numpy as np\n",
    "from docx import Document\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "%config Completer.use_jedi = False\n",
    "from pathlib import Path\n",
    "# import tqdm.notebook.tqdm as tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.basicConfig(format=u'%(filename)+13s [ LINE:%(lineno)-4s] %(levelname)-8s %(message)s',\n",
    "                    level=logging.DEBUG)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from docx2python import docx2python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PdfParser:\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_pdf_to_df(filename) -> list[pd.DataFrame]:\n",
    "        tables = camelot.read_pdf(str(filename), line_tol=2, joint_tol=10, line_scale=40, copy_text=['v','h'], pages='1-end') # , flavor='stream' row_tol=10\n",
    "        tables = [e.df for e in tables]\n",
    "        return tables\n",
    "\n",
    "    def get_camelot_tables(self, filename):\n",
    "        tables = camelot.read_pdf(str(filename), line_tol=2, joint_tol=100, line_scale=40, copy_text=['v'], pages='1-end') # , flavor='stream' row_tol=10\n",
    "        return tables\n",
    "\n",
    "\n",
    "# file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\pdf\\converted\\83301_2018_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii_(FGBU).pdf\"\n",
    "# pdf_parser = PdfParser()\n",
    "# test_dfs = pdf_parser.convert_pdf_to_df(file)\n",
    "# tables = pdf_parser.get_camelot_tables(file)\n",
    "# temp_dfs = pdf_parser.convert_pdf_to_df(file)\n",
    "# camelot.plot(tables[1], kind='textedge').show()\n",
    "# temp_dfs[2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\pdf\\converted\\83300_2018_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii_(FGUP).pdf\"\n",
    "\n",
    "# # pdf_parser = PdfParser()\n",
    "# # res = pdf_parser.convert_pdf_to_df(Path(file))\n",
    "# res[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CorrectHeadersParser:\n",
    "\n",
    "    '''класс для парсинга таблиц, у которых на месте колонки, которые нам нужны'''\n",
    "\n",
    "    def table_splitter(self, table: pd.DataFrame) -> list[pd.DataFrame]:\n",
    "        '''разделяет таблицы, в которых учреждение указано внутри таблицы'''\n",
    "\n",
    "        def check_if_same(my_array: list) -> bool:\n",
    "            \n",
    "            '''проверяем одинаковые ли колонки'''\n",
    "\n",
    "            if len(set(my_array))>1:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "            # first = my_array[0]\n",
    "            # for e in my_array[1:]:\n",
    "            #     if e != first:\n",
    "            #         return False\n",
    "            # return True\n",
    "\n",
    "        def get_indexes_to_split(table):\n",
    "            index_to_split = []\n",
    "            for e in range(len(table)):\n",
    "                cols = table.iloc[e,:].values\n",
    "                if check_if_same(cols):\n",
    "                    index_to_split.append(e)\n",
    "            return index_to_split\n",
    "\n",
    "\n",
    "        def split_table(table: pd.DataFrame, index_to_split:Union[int, list[int]]) -> list[pd.DataFrame]:\n",
    "            \"\"\"разделяет таблицу в случае когда название учреждения поместили в середину вот так:\n",
    "\n",
    "                -должность-  -имя-  -зарплата-\n",
    "                        -ГБОУ школа 112-\n",
    "                 директор     Ваня    100 руб\n",
    "\n",
    "             \"\"\"\n",
    "            dfs = np.array_split(table, index_to_split)\n",
    "            dfs = [e for e in dfs if len(e) > 0]\n",
    "\n",
    "            result_dfs = []\n",
    "            for df in dfs:\n",
    "                office = df.iloc[0,:][0]\n",
    "                df = df.iloc[1:,:] \n",
    "                df['office'] = office\n",
    "                result_dfs.append(df)\n",
    "            \n",
    "            result_dfs = [e for e in result_dfs if not e.empty]\n",
    "            try:\n",
    "                result_dfs = pd.concat(result_dfs)\n",
    "                return result_dfs\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                print('rogue file---', table)\n",
    "                \n",
    "        index_to_split = get_indexes_to_split(table)\n",
    "\n",
    "        if not index_to_split:\n",
    "            return table\n",
    "\n",
    "        splitted_dfs = split_table(table, index_to_split)\n",
    "        return splitted_dfs\n",
    "\n",
    "        \n",
    "    def concat_name(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''соединяем колонки ФИО, если они в разных'''\n",
    "        \n",
    "        if 'name' not in df.columns:\n",
    "            return df\n",
    "        \n",
    "        names_df = df['name']\n",
    "    \n",
    "        if isinstance(names_df, str) or isinstance(names_df, pd.Series):\n",
    "            return df  \n",
    "    \n",
    "        # TODO:\n",
    "        # дропнуть маленькую колонку\n",
    "\n",
    "\n",
    "        names = [' '.join(e) for e in names_df.values]     \n",
    "        \n",
    "        df.drop(columns=['name'], inplace=True)\n",
    "        df['name'] = names\n",
    "        return df\n",
    "\n",
    "\n",
    "    def parse(self, table: pd.DataFrame) -> pd.DataFrame:\n",
    "        # table = self.concat_name(table)\n",
    "        table = self.table_splitter(table)\n",
    "        return table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataCleaner:\n",
    "    \"\"\"убирает лишние данные\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_unwanted_symbols(df):        \n",
    "        # TODO: чистка всех колонок\n",
    "        df = df.applymap(lambda x: str(x).replace('\\n', ' '))\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_unwanted_cells(df):\n",
    "        print('КОЛОНКИ В remove_unwanted_cells ----- ', df.columns)\n",
    "        \n",
    "        # убирает ячейки с нумерацией\n",
    "        # print('--- DataCleaner.remove_unwanted_cells ---', df.columns)\n",
    "        #TODO: почему тут только должность?\n",
    "\n",
    "        # df = df[~df['position'].astype(str).str.isdigit()]\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_short_rows(df):\n",
    "        # удаляет ряды с недостаточными данными\n",
    "        # ! должно применяться после выбора норм колонок\n",
    "        to_remove = []\n",
    "        for tup in df.itertuples():\n",
    "            res = [len(str(e)) for e in tup]\n",
    "            if statistics.mean(res) < 5:\n",
    "                to_remove.append(tup.Index)\n",
    "        \n",
    "\n",
    "        df.drop(to_remove, inplace=True)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_salary(df: pd.DataFrame):\n",
    "        def converter(salary):\n",
    "            pass\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "    def merge_if_three_names(df:pd.DataFrame):\n",
    "        # TODO: !!!!\n",
    "        pass             \n",
    "\n",
    "    def clean_df(self, df):\n",
    "        df = self.remove_unwanted_symbols(df)\n",
    "        df = self.remove_unwanted_cells(df)\n",
    "\n",
    "        df = self.remove_short_rows(df)\n",
    "        # print('!!!',df)\n",
    "\n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DocxParser:\n",
    "\n",
    "    def get_docx_tables(self, filename, tab_id=None, **kwargs) -> list[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "            filename:   file name of a Word Document\n",
    "            tab_id:     parse a single table with the index: [tab_id] (counting from 0).\n",
    "                        When [None] - return a list of DataFrames (parse all tables)\n",
    "        \"\"\"\n",
    "        def read_docx_tab(tab, **kwargs):\n",
    "            vf = io.StringIO()\n",
    "            writer = csv.writer(vf)\n",
    "            for row in tab.rows:\n",
    "                writer.writerow(cell.text for cell in row.cells)\n",
    "            vf.seek(0)\n",
    "            return pd.read_csv(vf, **kwargs)\n",
    "\n",
    "        doc = Document(filename)\n",
    "        if tab_id is None:\n",
    "            return [read_docx_tab(tab, **kwargs) for tab in doc.tables]\n",
    "        else:\n",
    "            try:\n",
    "                return read_docx_tab(doc.tables[tab_id], **kwargs)\n",
    "            except IndexError:\n",
    "                print('Error: specified [tab_id]: {}  does not exist.'.format(tab_id))\n",
    "                raise\n",
    "            \n",
    "\n",
    "    def convert_docx_to_df(self, filename: str) -> pd.DataFrame:\n",
    "        assert filename.endswith('docx'), 'Формат должен быть .docx!'\n",
    "            \n",
    "        doc = Document(filename)\n",
    "        # TODO: тут взять текст, который потом прикрутить к\n",
    "\n",
    "        doc_tables = self.get_docx_tables(filename) \n",
    "        \n",
    "        return doc_tables\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cols_we_need = ['name','salary', 'position', 'department']\n",
    "        self.all_docs: list[str]\n",
    "        self.docx_parser = DocxParser()\n",
    "        self.pdf_parser = PdfParser()\n",
    "        self.parse_correct_headers = CorrectHeadersParser()\n",
    "        self.incorrect_headers_parser = IncorrectHeaders()\n",
    "        self.data_cleaner = DataCleaner()\n",
    "\n",
    "    @staticmethod\n",
    "    def rename_col(col: str) -> str:\n",
    "\n",
    "        print('col before rename cols --', col)\n",
    "        col = str(col).lower()\n",
    "        if re.search(pattern='(фамилия|имя|фио|ф\\.и\\.о\\.|ф\\.и\\.о|отчество)', string=col):\n",
    "            return \"name\"\n",
    "\n",
    "        elif re.search(pattern='(рублей|руб|cреднемесячная|зарпл.|плат[ы, а]|заработн[ой, ая] плат[а, ы]|cреднемесячн[ая, ой]|зарплат[а, ной, ы])', string=col):\n",
    "            return \"salary\"\n",
    "\n",
    "        elif re.search(pattern='(должност[ь, и, ей])', string=col): \n",
    "            return 'position'\n",
    "\n",
    "        elif re.search(pattern='(предприяти[е,я]|учреждени[е,я]|юридическое лицо)', string=col):\n",
    "            return 'department'\n",
    "\n",
    "        return col\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def check_if_columns_ok(cols: tuple) -> bool:\n",
    "        '''проверяем, есть ли в заголовках таблицы название предприятия и другая инфа'''\n",
    "        \n",
    "        cols = list(map(str, cols))\n",
    "        cols = list(map(str.lower, cols))\n",
    "        ok_cols = 0\n",
    "        company_found = False\n",
    "        for col in cols:\n",
    "            company_pattern = '(предприяти[е,я]|учреждени[е,я]|юридическ[ое,ие])'\n",
    "            res = re.search(pattern=company_pattern, string=col)            \n",
    "            if res:\n",
    "                company_found = True\n",
    "                continue\n",
    "                      \n",
    "            name_salary_position_pattern = '(фамилия|имя|фио|ф\\.и\\.о\\.|ф\\.и\\.о|отчество|плат[ы, а]|заработная|плата|cреднемесячн[ая, ой]|зарплат[а, ной, ы]|должность)'\n",
    "            res = re.search(pattern=name_salary_position_pattern, string=col)\n",
    "            if res:\n",
    "                ok_cols+=1\n",
    "\n",
    "        if company_found and ok_cols > 1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def parse_file(self, file: str):\n",
    "        \n",
    "        if str(file).endswith('.pdf'):\n",
    "            tables = self.pdf_parser.convert_pdf_to_df(file)\n",
    "        \n",
    "        elif file.endswith('docx'):\n",
    "            tables = self.docx_parser.convert_docx_to_df(file)\n",
    "\n",
    "        else:\n",
    "            logger.error('Допустимы расширения: pdf, docx')\n",
    "\n",
    "        parsed_tables = []\n",
    "\n",
    "        for table in tables:\n",
    "            # удалить кал из маленьких ячеек?\n",
    "                         \n",
    "            columns_ok = self.check_if_columns_ok(table)\n",
    "\n",
    "            if not columns_ok:\n",
    "                # колонки непправильные. идем пытаться найти нормальные. для этого мы весь файл передаем в Incorrect и скипаем цикл \n",
    "                \n",
    "                parsing_ok, tables = self.incorrect_headers_parser.parse(Path(file))\n",
    "                \n",
    "                if parsing_ok:\n",
    "                    for table in tables:\n",
    "                        # оставляем нужные колонки\n",
    "                        table.reset_index(inplace=True)\n",
    "                        table.columns = [self.rename_col(col) for col in table.columns]\n",
    "                        cols_to_leave = [col for col in table.columns if col in self.cols_we_need]\n",
    "                        cols_to_leave = set(cols_to_leave)\n",
    "                        table = table[cols_to_leave]\n",
    "                        # проверяем на наличие вложенных таблиц и фио, разнесенных на несколько стаоблцов\n",
    "                        table = self.parse_correct_headers.parse(table)\n",
    "                        # убираем лишние ячейки и символы\n",
    "                        \n",
    "                        #table = self.data_cleaner.clean_df(table)\n",
    "                        parsed_tables.append(table)\n",
    "                    break\n",
    "\n",
    "                else:                    \n",
    "                    # TODO: сохранить файл в папку нераспаршенных \n",
    "                    logger.warning('Не удалось распарсить ----', file)\n",
    "\n",
    "            elif columns_ok:                \n",
    "                # если заголовки ок \n",
    "                # оставляем только нужные колонки \n",
    "                table.reset_index(inplace=True)        \n",
    "                table.columns = [self.rename_col(col) for col in table.columns]\n",
    "                cols_to_leave = [col for col in table.columns if col in self.cols_we_need]\n",
    "                cols_to_leave = set(cols_to_leave)\n",
    "                table = table[cols_to_leave]\n",
    "                # проверяем на наличие вложенных таблиц и фио, разнесенных на несколько стаоблцов\n",
    "                table = self.parse_correct_headers.parse(table)\n",
    "                # убираем лишние ячейки и символы\n",
    "                # table = self.data_cleaner.clean_df(table)\n",
    "                parsed_tables.append(table)\n",
    "\n",
    "\n",
    "        if isinstance(parsed_tables, list):\n",
    "            if parsed_tables:\n",
    "                concat_tables = pd.concat(parsed_tables)\n",
    "                return concat_tables\n",
    "    \n",
    "        elif isinstance(parsed_tables, pd.DataFrame):\n",
    "            if not parsed_tables.empty:\n",
    "                return concat_tables\n",
    "        \n",
    "\n",
    "\n",
    "base = 'data_ids/pdf/converted/'\n",
    "file = '189273_2020_Rektor,_prorektory,_glavnyi_bukhgalter.pdf'\n",
    "file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\pdf\\converted\\100185_2019_Rukovoditeli_podvedomstvennykh_uchrezhdenii_(sport).pdf\"\n",
    "\n",
    "parser = Parser()\n",
    "folder = Path(r'D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\pdf\\converted\\для_которых_нужен_перевод_в_ворд')\n",
    "\n",
    "file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\pdf\\converted\\83289_2018_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii.pdf\"\n",
    "\n",
    "# res = parser.parse_file(file)\n",
    "# res\n",
    "# for e in os.listdir(folder):\n",
    "#     if e.endswith('.pdf'):\n",
    "#         res = parser.parse_file(folder / e)\n",
    "#         with open(e+'.pkl','wb') as f:\n",
    "#             pickle.dump(res, f)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# extract docx content\n",
    "file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\docx\\102585_2019_Rukovoditeli_podvedomstvennykh_uchrezhdenii_(kul'tura).docx\"\n",
    "# file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\docx\\101232_2018_Rukovoditeli_podvedomstvennykh_uchrezhdenii_(obrazovanie).docx\"\n",
    "file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\docx\\184287_2018_Rukovoditeli_podvedomstvennykh_uchrezhdenii_(obrazovanie).docx\"\n",
    "# file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\docx\\102585_2019_Rukovoditeli_podvedomstvennykh_uchrezhdenii_(kul'tura).docx\"\n",
    "file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\docx\\179512_2020_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii.docx\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class IncorrectHeaders:\n",
    "    \"\"\"класс для таблиц с неопределенными заголовками.\n",
    "        1. Пытаемся найти название учреждений в объединенных ячейках.\n",
    "        2. Если не получается, для учреждения берем текст, предшествующий таблице. \n",
    "    \"\"\"\n",
    "\n",
    "    # если прошелся по таблице и нашел вложения внутри - пусть это будет офис.\n",
    "    # если не нашел - берем название офиса из абзацев вокруг таблиц (если их число плюс минус совпадает)\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        # TODO: добавить обработку чисто docx \n",
    "\n",
    "        # self.docx_parser = DocxParser()\n",
    "        # self.pdf_parser = PdfParser()\n",
    "    @staticmethod\n",
    "    def drop_col_with_N(df:pd.DataFrame):\n",
    "        expr = '(№|п/п)'\n",
    "        for c in df.columns:\n",
    "    \n",
    "            if re.search(expr, str(c)):\n",
    "                df.drop(columns=c, inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_short_cols(df: pd.DataFrame):\n",
    "        df = df.applymap(str)\n",
    "        len_df = df.applymap(len)\n",
    "        columns_numbers = [x for x in range(df.shape[1])] \n",
    "        to_remove = []\n",
    "        for i in columns_numbers:\n",
    "            if len_df.iloc[:,i].mean() < 4:\n",
    "                to_remove.append(i)\n",
    "        if to_remove:\n",
    "            for e in to_remove:\n",
    "                columns_numbers.remove(e)\n",
    "\n",
    "        return df.iloc[:, columns_numbers]\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_short_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        for i in range(3):\n",
    "            cols = list(map(str, df.columns)) \n",
    "            cols = list(map(len, cols)) \n",
    "            if statistics.mean(cols) < 3 and i<2:\n",
    "                df.columns = df.iloc[0,:]\n",
    "            else:\n",
    "                return df\n",
    "        \n",
    "\n",
    "\n",
    "    def table_splitter(self, table: pd.DataFrame) -> tuple[bool, list[pd.DataFrame]]:\n",
    "        '''разделяет таблицы, в которых учреждение указано внутри таблицы'''\n",
    "\n",
    "        def check_if_same(my_array: list) -> bool:\n",
    "            '''проверяем одинаковые ли колонки'''\n",
    "\n",
    "            if len(set(my_array))>1:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "\n",
    "        def get_indexes_to_split(table):\n",
    "            '''определяем индекс строки таблицы, по которому надо разделить'''\n",
    "\n",
    "            index_to_split = []\n",
    "            for e in range(len(table)):\n",
    "                cols = table.iloc[e,:].values\n",
    "                if check_if_same(cols):\n",
    "                    index_to_split.append(e)\n",
    "            return index_to_split\n",
    "\n",
    "                                                                                                #если разделили и нашли офис - True \n",
    "        def split_table(table: pd.DataFrame, index_to_split:Union[int, list[int]]) -> tuple[bool, list[pd.DataFrame]]:\n",
    "            \"\"\"разделяет таблицу в случае когда название учреждения поместили в середину вот так:\n",
    "                -должность-  -имя-  -зарплата-\n",
    "                        -ГБОУ школа 112-\n",
    "                 директор     Ваня    100 руб\n",
    "\n",
    "             \"\"\"\n",
    "            dfs = np.array_split(table, index_to_split)\n",
    "            dfs = [e for e in dfs if len(e) > 0]\n",
    "\n",
    "            result_dfs = []\n",
    "            for df in dfs:\n",
    "                office = df.iloc[0,:][0]\n",
    "                df = df.iloc[1:,:] \n",
    "                df['office'] = office\n",
    "                result_dfs.append(df)\n",
    "            \n",
    "            result_dfs = [e for e in result_dfs if not e.empty]\n",
    "            try:\n",
    "                result_dfs = pd.concat(result_dfs)\n",
    "                return result_dfs\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                # print('rogue file---', table)\n",
    "                \n",
    "        index_to_split = get_indexes_to_split(table)\n",
    "\n",
    "        if not index_to_split:\n",
    "            return False, table\n",
    "\n",
    "        splitted_dfs = split_table(table, index_to_split)\n",
    "        return True, splitted_dfs\n",
    "\n",
    "\n",
    "    def convert_pdf_to_docx_to_find_info(self, filename: Path) -> Path:\n",
    "        # переводим пфд в ворд\n",
    "        assert str(filename).endswith('.pdf'), 'Файл должен быть в PDF !'\n",
    "        folder = filename.parents[0]\n",
    "\n",
    "        orig_file_name = filename.name.strip('.pdf')                 \n",
    "        new_name = 'temp_to_delete_' + orig_file_name + '.docx'\n",
    "\n",
    "        pdf2docx.parse(str(filename), str(folder / new_name))\n",
    "        return folder / new_name\n",
    "\n",
    "    @staticmethod\n",
    "    def check_if_columns_ok(cols: tuple) -> bool:\n",
    "        '''проверяем, есть ли в заголовках таблицы нужная инфа'''\n",
    "        \n",
    "        cols = list(map(str, cols))\n",
    "        cols = list(map(str.lower, cols))\n",
    "\n",
    "        ok_cols = 0\n",
    "        for col in cols:\n",
    "            name_salary_position_pattern = '(фамилия|имя|фио|ф\\.и\\.о\\.|ф\\.и\\.о|отчество|плат[ы, а]|заработная|плата|cреднемесячн[ая, ой]|зарплат[а, ной, ы]|должность)'\n",
    "            \n",
    "            res = re.search(pattern=name_salary_position_pattern, string=col)\n",
    "            if res:\n",
    "                ok_cols+=1\n",
    "\n",
    "        if ok_cols > 1:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "    def find_ok_cols(self, df:pd.DataFrame) -> dict['df':pd.DataFrame, 'if_ok_cols':bool]:\n",
    "\n",
    "        cols = df.columns\n",
    "       # если колонки норм, отдаем df\n",
    "        if self.check_if_columns_ok(cols):\n",
    "            return {'df':df, 'if_ok_cols':True}\n",
    "\n",
    "        i = -1\n",
    "        for _, row in df.iterrows():\n",
    "            i+=1\n",
    "            found_cols =  self.check_if_columns_ok(list(row))   \n",
    "\n",
    "            if found_cols:\n",
    "                df.columns = df.iloc[i,:]\n",
    "                return {'df':df.iloc[i+1:,:], 'if_ok_cols':True}\n",
    "\n",
    "            if i > 5:\n",
    "                break\n",
    "        \n",
    "        # если не ок\n",
    "        return {'df':df,'if_ok_cols':False}\n",
    "    \n",
    "    @staticmethod\n",
    "    def if_office_in_cols(dfs: list[pd.DataFrame]) -> bool:\n",
    "        for df in dfs:\n",
    "            cols = df.columns\n",
    "\n",
    "            cols = list(map(str, cols))\n",
    "            cols = list(map(str.lower, cols))\n",
    "\n",
    "            office_pattern = '(предприяти[е,я]|учреждени[е,я]|юридическ[ие, ое])'\n",
    "\n",
    "            if not any([re.search(pattern=office_pattern, string=col) for col in cols]):\n",
    "                return False\n",
    "\n",
    "        return True        \n",
    "\n",
    "    def convert_pdf_to_dfs(self, filename: str) -> list[pd.DataFrame]:\n",
    "\n",
    "        try:\n",
    "            tables = camelot.read_pdf(str(filename), line_tol=2, joint_tol=10, line_scale=40, copy_text=[\n",
    "                                     'v','h'], pages='1-end') # , flavor='stream' row_tol=10\n",
    "        \n",
    "            tables = [e.df for e in tables]\n",
    "            return tables\n",
    "\n",
    "        except Exception as ex:\n",
    "            logger.error('file --- %s', filename)\n",
    "            logger.error('Exception --- %s', ex)\n",
    "\n",
    "\n",
    "    def detect_headers_in_raw_doc(self, filename, parsed_tables: list[pd.DataFrame]) -> list[pd.DataFrame]:\n",
    "        \n",
    "        def get_headers(filename: str) -> list[str]: #filename:docx\n",
    "\n",
    "            doc = docx2python(filename)\n",
    "\n",
    "            table_pattern = '(фамилия|имя|фио|ф\\.и\\.о\\.|ф\\.и\\.о|отчество|должность)'\n",
    "\n",
    "            offices = []\n",
    "            gathering_office_info = ''\n",
    "\n",
    "            for paragraph in doc.body_runs: #параграфы в виде вложенных листов\n",
    "\n",
    "                paragraph = sum(sum(paragraph, []), [])\n",
    "                paragraph_text = ''\n",
    "                for e in paragraph:\n",
    "                    try:\n",
    "                        paragraph_text += ' ' + e[0] + ' '\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                \n",
    "                paragraph_text = paragraph_text.lower()\n",
    "                its_table = re.findall(pattern=table_pattern, string=paragraph_text)\n",
    "                \n",
    "                if not its_table:\n",
    "                    gathering_office_info+=paragraph_text\n",
    "                \n",
    "                elif its_table:\n",
    "                    offices.append(gathering_office_info)\n",
    "                    gathering_office_info=''\n",
    "\n",
    "            return [e for e in offices if e]\n",
    "\n",
    "        def compile_office_info_and_df(filename: Path, departments: list, tables: list[pd.DataFrame]) -> typing.Union[None, list[pd.DataFrame]]:\n",
    "            # все правильно. логика такая, что камелотом лучше парсить!\n",
    "            # а док только для загов таблиц\n",
    "\n",
    "            # tables = self.convert_pdf_to_dfs(filename)\n",
    "\n",
    "            ok_dfs = []\n",
    "\n",
    "            print('Количество заголовков --- ', len(departments))\n",
    "            print('Количество таблиц --- ', len(tables))\n",
    "            \n",
    "            if len(departments) - len(tables) == 1:\n",
    "                departments.pop()\n",
    "\n",
    "\n",
    "            if len(departments) == len(tables):\n",
    "                for table, dep in zip(tables, departments):\n",
    "                    table['Учреждение'] = dep\n",
    "                    table['Учреждение'][0] = 'Учреждение'\n",
    " \n",
    "                    ok_dfs.append(table)\n",
    "          \n",
    "                return ok_dfs\n",
    "            \n",
    "\n",
    "            with open(str(filename) + '.txt', 'w') as f:\n",
    "                text = f'Разное число таблиц ({len(tables)}) и учреждений ({len(departments)})'\n",
    "                f.write(text)\n",
    "\n",
    "#            raise ValueError(f'Разное число таблиц ({len(tables)}) и учреждений ({len(departments)})')\n",
    "\n",
    "        temp_docfile = self.convert_pdf_to_docx_to_find_info(filename) # получили path временного docx файла\n",
    "        departments = get_headers(temp_docfile)\n",
    "        dfs = compile_office_info_and_df(filename, departments, parsed_tables)\n",
    "        return dfs\n",
    "\n",
    "    def concatenate_if_possible(self, dfs: list[dict['df':pd.DataFrame, 'if_ok_cols':bool]]) -> list[pd.DataFrame]:\n",
    "        \n",
    "        all_oks = [e['if_ok_cols'] for e in dfs]\n",
    "        \n",
    "        if all(all_oks):\n",
    "            return [e['df'] for e in dfs]\n",
    "\n",
    "        result_df = []\n",
    "        df_to_concat = pd.DataFrame()\n",
    "        for df_info in dfs:\n",
    "            if df_info['if_ok_cols']:\n",
    "                if not df_to_concat.empty:\n",
    "                    result_df.append(df_to_concat)\n",
    "                df_to_concat = df_info['df']\n",
    "            \n",
    "            # оставляем только таблицы, у которых совпадает число колонок\n",
    "            # с df у которых мы колонки нашли \n",
    "            # если не нашли колонки и не к чему присоединять - дропаем\n",
    "\n",
    "            elif not df_info['if_ok_cols'] and not df_to_concat.empty \\\n",
    "                and len(df_to_concat.columns) == len(df_info['df'].columns):\n",
    "                df_info['df'].columns = df_to_concat.columns\n",
    "                df_to_concat = pd.concat([df_to_concat, df_info['df']])\n",
    "        \n",
    "        result_df.append(df_to_concat)\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "\n",
    "    def parse(self, filename: Path) -> tuple[bool, pd.DataFrame]: # Path - относительный\n",
    "        # пытаемся найти учреждения в теле таблиц\n",
    "\n",
    "        #TODO: добавить проверку doc или pdf\n",
    "\n",
    "        # должны быть просто таблицы\n",
    "        # и вся обработка должна быть тут, по этапам. иначе макароны\n",
    "        tables = self.convert_pdf_to_dfs(filename)\n",
    "\n",
    "        \n",
    "        # дропаем маленькие колонки\n",
    "        tables = [self.drop_short_headers(e) for e in tables]\n",
    "        tables = [self.drop_col_with_N(e) for e in tables]\n",
    "        tables = [e for e in tables if type(e) == pd.DataFrame]\n",
    "        # with open('drop_test.pkl', 'wb') as f:\n",
    "        #     pickle.dump(tables, f)\n",
    "        tables = [self.drop_short_cols(e) for e in tables]\n",
    "        \n",
    "        \"\"\"\n",
    "        Есть все таблицы. У некоторых нет вообще заголовков. \n",
    "        Присоединяем их к тем у кого есть заголовки.\n",
    "        Получаем таблицы.\n",
    "        Если у них нет учреждений - идем парсить в док. \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # у нас тут лист словарей {df:bool}. к каждой таблице мы должны приделать True или False\n",
    "        tables = [self.find_ok_cols(e) for e in tables]\n",
    "        # TODO: если нет ни одной таблицы с ок загами -> скипаем все\n",
    "        \n",
    "        at_least_one_table_ok = any([e['if_ok_cols'] for e in tables])\n",
    "        if not at_least_one_table_ok:\n",
    "            return False, []\n",
    "\n",
    "        # теперь надо склеить таблицы, если есть таблицы с ок колонками\n",
    "        tables = self.concatenate_if_possible(tables)\n",
    "        \n",
    "        # проверяем есть ли учреждение\n",
    "        if self.if_office_in_cols(tables):\n",
    "            return True, tables\n",
    "        \n",
    "        # если нет - парсим док. \n",
    "\n",
    "        tables_with_ok_headers = []\n",
    "        \n",
    "        for table in tables:\n",
    "            res, df = self.table_splitter(table)\n",
    "            if res:\n",
    "                tables_with_ok_headers.append(df)\n",
    "\n",
    "            if not res:\n",
    "                # идем парсить весь док, чтобы достать учреждения из текста перед таблицей\n",
    "                dfs = self.detect_headers_in_raw_doc(filename, parsed_tables=tables)\n",
    "                if not dfs:\n",
    "                    return False, []\n",
    "                for df in dfs:\n",
    "                    tables_with_ok_headers.append(df)\n",
    "                break\n",
    "\n",
    "        #TODO: переделать удаление временного дока\n",
    "        return bool(tables_with_ok_headers), tables_with_ok_headers        \n",
    "\n",
    "\n",
    "       # 'temp_to_delete_D:\\\\PROGR\\\\LEARN_PYTHON\\\\Declarator\\\\declarations-parser\\\\data_ids\\\\pdf\\\\converted\\\\для которых нужен перевод в ворд\\\\88766_2019_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii.docx'\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\pdf\\converted\\83301_2018_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii_(FGBU).pdf\"\n",
    "file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\pdf\\converted\\83333_2018_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii.pdf\"\n",
    "\n",
    "file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\pdf\\83334_2018_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii.pdf\"\n",
    "\n",
    "# res = parser.parse_file(Path(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>department</th>\n",
       "      <th>position</th>\n",
       "      <th>salary</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Учреждение</td>\n",
       "      <td>На11менование должности</td>\n",
       "      <td>Среднемесячная \\nзаработная плата, \\nруб.</td>\n",
       "      <td>Фамилия, имя, отчество</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>----media/image1.png----  информация о рассчи...</td>\n",
       "      <td>Ректор</td>\n",
       "      <td>565  838,0</td>\n",
       "      <td>Безбородов Александр Борисович</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>----media/image1.png----  информация о рассчи...</td>\n",
       "      <td>Первый проректор-\\nпроректор по научной \\nработе</td>\n",
       "      <td>576 061,2</td>\n",
       "      <td>Павленко Ольга Вячеславовна</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>----media/image1.png----  информация о рассчи...</td>\n",
       "      <td>Проректор по учебной \\nработе</td>\n",
       "      <td>445  242,4</td>\n",
       "      <td>Архипова Надежда Ивановна</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>----media/image1.png----  информация о рассчи...</td>\n",
       "      <td>Проректор по социальной \\nи воспитательной раб...</td>\n",
       "      <td>. \\n398  165,9</td>\n",
       "      <td>Болквадзе Ираклий Ревазович</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>----media/image1.png----  информация о рассчи...</td>\n",
       "      <td>Проректор по \\nбезопасности</td>\n",
       "      <td>391  153,5</td>\n",
       "      <td>Трифонов Николай Николаевич</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>----media/image1.png----  информация о рассчи...</td>\n",
       "      <td>Проректор по \\nмеждународному \\nсотрудничеству</td>\n",
       "      <td>456 800,6</td>\n",
       "      <td>Заботкина Вера Ивановна</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>----media/image1.png----  информация о рассчи...</td>\n",
       "      <td>Проректор по развитию и \\nинформационным \\nрес...</td>\n",
       "      <td>343  738,8</td>\n",
       "      <td>Кожокин Михаил Михайлович</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>----media/image1.png----  информация о рассчи...</td>\n",
       "      <td>Проректор по проектной \\nдеятельности</td>\n",
       "      <td>362 715,6</td>\n",
       "      <td>Пастухова Лариса Сергеевна</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>----media/image1.png----  информация о рассчи...</td>\n",
       "      <td>Проректор по \\nнепрерывному \\nобразованию</td>\n",
       "      <td>470 015,2</td>\n",
       "      <td>Шкаренков Павел Петрович</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>----media/image1.png----  информация о рассчи...</td>\n",
       "      <td>Главный бухгалтер</td>\n",
       "      <td>432 290,2</td>\n",
       "      <td>Третьякова Светлана Алексеевна</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           department  \\\n",
       "0                                          Учреждение   \n",
       "1    ----media/image1.png----  информация о рассчи...   \n",
       "2    ----media/image1.png----  информация о рассчи...   \n",
       "3    ----media/image1.png----  информация о рассчи...   \n",
       "4    ----media/image1.png----  информация о рассчи...   \n",
       "5    ----media/image1.png----  информация о рассчи...   \n",
       "6    ----media/image1.png----  информация о рассчи...   \n",
       "7    ----media/image1.png----  информация о рассчи...   \n",
       "8    ----media/image1.png----  информация о рассчи...   \n",
       "9    ----media/image1.png----  информация о рассчи...   \n",
       "10   ----media/image1.png----  информация о рассчи...   \n",
       "\n",
       "                                             position  \\\n",
       "0                             На11менование должности   \n",
       "1                                              Ректор   \n",
       "2    Первый проректор-\\nпроректор по научной \\nработе   \n",
       "3                       Проректор по учебной \\nработе   \n",
       "4   Проректор по социальной \\nи воспитательной раб...   \n",
       "5                         Проректор по \\nбезопасности   \n",
       "6      Проректор по \\nмеждународному \\nсотрудничеству   \n",
       "7   Проректор по развитию и \\nинформационным \\nрес...   \n",
       "8               Проректор по проектной \\nдеятельности   \n",
       "9           Проректор по \\nнепрерывному \\nобразованию   \n",
       "10                                  Главный бухгалтер   \n",
       "\n",
       "                                       salary                            name  \n",
       "0   Среднемесячная \\nзаработная плата, \\nруб.          Фамилия, имя, отчество  \n",
       "1                                  565  838,0  Безбородов Александр Борисович  \n",
       "2                                   576 061,2     Павленко Ольга Вячеславовна  \n",
       "3                                  445  242,4       Архипова Надежда Ивановна  \n",
       "4                              . \\n398  165,9     Болквадзе Ираклий Ревазович  \n",
       "5                                  391  153,5     Трифонов Николай Николаевич  \n",
       "6                                   456 800,6         Заботкина Вера Ивановна  \n",
       "7                                  343  738,8       Кожокин Михаил Михайлович  \n",
       "8                                   362 715,6      Пастухова Лариса Сергеевна  \n",
       "9                                   470 015,2        Шкаренков Павел Петрович  \n",
       "10                                  432 290,2  Третьякова Светлана Алексеевна  "
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\pdf\\converted\\189551_2020_Rektor,_prorektory,_glavnyi_bukhgalter.pdf\"\n",
    "parser = Parser()\n",
    "# res = parser.parse_file(Path(file))\n",
    "res\n",
    "# cleaner = DataCleaner()\n",
    "# cleaner.clean_df(res)\n",
    "\n",
    "#incor_parser = IncorrectHeaders()\n",
    "#okli, tables = incor_parser.parse(Path(file))\n",
    "# parser.rename_col(tables[0].columns)\n",
    "#table = tables[0]\n",
    "\n",
    "\n",
    "# table.columns = [parser.rename_col(col) for col in table.columns]\n",
    "# cols_to_leave = [col for col in table.columns if col in parser.cols_we_need]\n",
    "# cols_to_leave = set(cols_to_leave)\n",
    "# table = table[cols_to_leave]\n",
    "# test_df = parser.parse_correct_headers.parse(table)\n",
    "# parser.data_cleaner.clean_df(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = r'D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser'\n",
    "# files_to_parse = []\n",
    "# for file in os.listdir(folder):\n",
    "#     if file.endswith('.txt'):\n",
    "#         with open(f'../{file}', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "#             text = ''.join(f.readlines())\n",
    "#             if \"AttributeError: 'NoneType' object has no attribute 'applymap\" in text:\n",
    "#                 files_to_parse.append(file)\n",
    "\n",
    "#files_to_parse = [e.strip('error_').strip('.txt') for e in files_to_parse]\n",
    "# with open('files_to_parse.txt', 'w') as f:\n",
    "#     f.write('\\n'.join(files_to_parse))\n",
    "\n",
    "\n",
    "# for file in to_remove:\n",
    "#     os.remove(f'../{file}')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\pdf\\converted\\temp\\Итог1.pdf\"\n",
    "# file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\pdf\\83300_2018_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii_(FGUP).pdf\"\n",
    "# pdf_parser = PdfParser()\n",
    "# tables = pdf_parser.convert_pdf_to_df(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10704/2839466029.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mdoc_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\pdf\\converted\\104563_2019_Rektor,_prorektory,_glavnyi_bukhgalter.docx\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocx2python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody_runs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#параграфы в виде вложенных листов\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mparagraph_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'temp_file' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_headers(doc_file):\n",
    "\n",
    "    doc = docx2python(doc_file)\n",
    "\n",
    "    table_pattern = '(фамилия|имя|фио|ф\\.и\\.о\\.|ф\\.и\\.о|отчество|должность)'\n",
    "\n",
    "    offices = []\n",
    "    gathering_office_info = ''\n",
    "\n",
    "    for paragraph in doc.body_runs: #параграфы в виде вложенных листов\n",
    "\n",
    "        paragraph = sum(sum(paragraph, []), [])\n",
    "        paragraph_text = ''\n",
    "        for e in paragraph:\n",
    "            try:\n",
    "                paragraph_text += ' ' + e[0] + ' '\n",
    "            except IndexError:\n",
    "                pass\n",
    "        \n",
    "        paragraph_text = paragraph_text.lower()\n",
    "        its_table = re.findall(pattern=table_pattern, string=paragraph_text)\n",
    "        print(paragraph_text)\n",
    "        print('===')\n",
    "        if not its_table:\n",
    "            gathering_office_info+=paragraph_text\n",
    "        \n",
    "        elif its_table:\n",
    "            offices.append(gathering_office_info)\n",
    "            gathering_office_info=''\n",
    "\n",
    "    return offices\n",
    "\n",
    "# doc_file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\pdf\\converted\\для_которых_нужен_перевод_в_ворд\\temp_to_delete_89001_2019_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii.docx\"\n",
    "doc_file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\pdf\\converted\\100185_2019_Rukovoditeli_podvedomstvennykh_uchrezhdenii_(sport).docx\"\n",
    "doc_file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\pdf\\converted\\104563_2019_Rektor,_prorektory,_glavnyi_bukhgalter.docx\"\n",
    "\n",
    "doc = docx2python(temp_file)\n",
    "for paragraph in doc.body_runs: #параграфы в виде вложенных листов\n",
    "    paragraph_text = ''\n",
    "\n",
    "    paragraph = sum(sum(paragraph, []), [])\n",
    "    for e in paragraph:\n",
    "        try:\n",
    "            paragraph_text += ' ' + e[0] + ' '\n",
    "        except IndexError:\n",
    "            print('Ошибка!')\n",
    "            \n",
    "\n",
    "    print(paragraph_text)\n",
    "    print('===')\n",
    "\n",
    "# paragraph_text\n",
    "\n",
    "# offices = get_headers(temp_file)\n",
    "# offices\n",
    "\n",
    "# len([e for e in offices if e])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1664/3625192534.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m# dfs[1][0].head(12)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;31m# dfs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Parser' is not defined"
     ]
    }
   ],
   "source": [
    "# t = incor_parser.drop_short_headers(test_dfs[1])\n",
    "\n",
    "# t\n",
    "# # test_dfs[1]\n",
    "\n",
    "\n",
    "def drop_short_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for i in range(3):\n",
    "        cols = list(map(str, df.columns)) \n",
    "        cols = list(map(len, cols)) \n",
    "        if statistics.mean(cols) < 3 and i<2:\n",
    "            df.columns = df.iloc[0,:]\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "# df = tables[1]\n",
    "\n",
    "# df.columns = df.iloc[0,:]\n",
    "# df = df.iloc[1:,:]\n",
    "# cols = list(map(str, df.columns)) \n",
    "# cols = list(map(len, cols)) \n",
    "# statistics.mean(cols) < 3\n",
    "# droped_df = drop_short_headers(df)\n",
    "# droped_df\n",
    "\n",
    "# tables = [incor_parser.drop_short_headers(e) for e in tables]\n",
    "# tables[1].columns\n",
    "\n",
    "# tables = [incor_parser.drop_col_with_N(e) for e in tables]\n",
    "\n",
    "\n",
    "# tables = [e for e in tables if type(e) == pd.DataFrame]\n",
    "\n",
    "\n",
    "# # tables = [incor_parser.drop_short_cols(e) for e in tables]\n",
    "\n",
    "\n",
    "# tables = [incor_parser.find_ok_cols(e) for e in tables]\n",
    "\n",
    "\n",
    "# at_least_one_table_ok = any([e['if_ok_cols'] for e in tables])\n",
    "\n",
    "\n",
    "# # теперь надо склеить таблицы, если есть таблицы с ок колонками\n",
    "# tables = incor_parser.concatenate_if_possible(tables)\n",
    "\n",
    "#tables[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file = r\"D:\\PROGR\\LEARN_PYTHON\\Declarator\\declarations-parser\\data_ids\\pdf\\83300_2018_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii_(FGUP).pdf\"\n",
    "#dfs = incor_parser.parse(Path(file))\n",
    "# dfs[1][0].head(12)\n",
    "# dfs\n",
    "parser = Parser()\n",
    "res = parser.parse_file(Path(file))\n",
    "res\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "137dc9a7529e050f0d3404627e217fd4120b6229f927507e6c7e6283ad4b3698"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
