{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b0bbdc6-a22f-42b0-80f2-50d15f8f0284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pdf', 'xlsx', 'docx', 'doc', 'xls', 'zip', 'rtf', 'PDF'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from typing import Union\n",
    "import csv\n",
    "\n",
    "import io\n",
    "import numpy as np\n",
    "from docx import Document\n",
    "\n",
    "import pandas as pd\n",
    "%config Completer.use_jedi = False\n",
    "from pathlib import Path\n",
    "# import tqdm.notebook.tqdm as tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "# from tqdm.notebook import tqdm\n",
    "df = pd.read_csv('salaries_with_ext.csv')\n",
    "df.head()\n",
    "df['extension'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc265770-b9db-4945-954d-46551590d17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "не вышло-- ~$292_2016_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii.docx\n",
      "не вышло-- ~$320_2017_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii.docx\n",
      "не вышло-- ~$336_2017_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii_(ne_ukazany_FIO).docx\n",
      "не вышло-- ~$339_2017_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii_(FGUP_GRChTs).docx\n",
      "не вышло-- ~$350_2017_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii_(ne_ukazany_FIO).docx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#doc = read_docx_tables(folder + '2017_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii_(ne_ukazany_FIO).docx')\n",
    "\n",
    "'''\n",
    "ФИО Зарплата Должность\n",
    "'''\n",
    "\n",
    "\n",
    "def rename_col(col):\n",
    "    col = col.lower()\n",
    "    if re.search(pattern='(фамилия|имя|фио|ф.и.о.|отчество)', string=col):\n",
    "        return \"ФИО\"\n",
    "\n",
    "    elif re.search(pattern='(заработная плата|cреднемесячная|зарпл.)', string=col):\n",
    "        return \"Зарплата\"\n",
    "\n",
    "    elif re.search(pattern='(должность)', string=col):\n",
    "        return 'Должность'\n",
    "\n",
    "    elif re.search(pattern='(предприяти[е,я]|учреждени[е,я]|юридическое лицо)', string=col):\n",
    "        return 'Учреждение'\n",
    "\n",
    "    \n",
    "    return col\n",
    "\n",
    "\n",
    "def check_if_columns_ok(cols: tuple) -> bool:\n",
    "    cols = list(map(str, cols))\n",
    "    cols = list(map(str.lower, cols))\n",
    "    one_already_found = False \n",
    "    pattern = '(фамилия|имя|фио|ф.и.о|отчество|заработная плата|cреднемесячная|зарпл|должность|(предприяти[е,я]|учреждени[е,я]))'\n",
    "    \n",
    "    for col in cols:\n",
    "        res = re.search(pattern=pattern, string=col)\n",
    "        if res and not one_already_found:\n",
    "            one_already_found = True\n",
    "        elif res and one_already_found:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "    \n",
    "def detect_and_rename_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    cols = df.columns\n",
    "    res = check_if_columns_ok(cols)\n",
    "\n",
    "    if not res:        \n",
    "        for index, cell in enumerate(df[:3].itertuples()):\n",
    "            res = check_if_columns_ok(cell)\n",
    "            if res:\n",
    "                df.columns = df.iloc[index]\n",
    "                df = df[index+1:]\n",
    "                break\n",
    "        if not res:\n",
    "            return False, df\n",
    "            #raise ValueError('заголовки колонок не найдены')\n",
    "\n",
    "    cols = df.columns\n",
    "    renamed_cols = []\n",
    "    for col in cols:\n",
    "        renamed_cols.append(rename_col(col))\n",
    "\n",
    "    df.columns = renamed_cols\n",
    "    return True, df\n",
    "\n",
    "\n",
    "def read_all_docs()->list[dict[str, Document]]:\n",
    "    all_docs = []\n",
    "    for doc in os.listdir(folder):\n",
    "        if not doc.endswith('docx'):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            all_docs.append({'file':doc,'doc':Document(folder + doc)})\n",
    "        except:\n",
    "            print('не вышло--', doc)\n",
    "    \n",
    "    return all_docs\n",
    "\n",
    "def get_all_tables(all_docs: list[dict]) -> list[pd.DataFrame]:\n",
    "    all_tables = []\n",
    "    for doc_dict in all_docs:\n",
    "        tables = read_docx_tables(folder + doc_dict['file'])        \n",
    "        all_tables.append(tables)\n",
    "\n",
    "    return all_tables\n",
    "\n",
    "#all_docs = read_all_docs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9a86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd9cd1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           department  salary  \\\n",
      "0                                                   1     4\\n   \n",
      "1   ГАПОУ РК «Колледж технологии и предприниматель...  66 582   \n",
      "2   ГАПОУ РК «Колледж технологии и предприниматель...  62 510   \n",
      "3   ГАПОУ РК «Колледж технологии и предприниматель...  46 746   \n",
      "4   ГАПОУ РК «Колледж технологии и предприниматель...  15 489   \n",
      "..                                                ...     ...   \n",
      "89  ГКУ РК «Хозяйственно – эксплуатационная группа...  76 044   \n",
      "90  ГКУ РК «Хозяйственно – эксплуатационная группа...  73 095   \n",
      "91  ГАУ РК «Карельский региональный центр молодежи»\\n  80 692   \n",
      "92  ГАУ РК «Карельский региональный центр молодежи»\\n  61 500   \n",
      "93  ГАУ РК «Карельский региональный центр молодежи»\\n  41 508   \n",
      "\n",
      "                         position                                 position  \\\n",
      "0                               3                                        3   \n",
      "1                     Директор \\n                              Директор \\n   \n",
      "2               Главный бухгалтер                        Главный бухгалтер   \n",
      "3          Заместитель директора                    Заместитель директора    \n",
      "4          Заместитель директора                    Заместитель директора    \n",
      "..                            ...                                      ...   \n",
      "89       Пашкова Елена Георгиевна                                Директор    \n",
      "90  Третьякова Ирина Владимировна                    Заместитель директора   \n",
      "91     Хохлов Эдуард Владимирович                                 Директор   \n",
      "92       Билькова Вера Алексеевна  Заместитель директора- начальник отдела   \n",
      "93           Герасев Илья Юрьевич              Руководитель центра -отдела   \n",
      "\n",
      "                                                 name  \n",
      "0                                                 2 2  \n",
      "1   Германова Галина Николаевна Германова Галина Н...  \n",
      "2   Ефименко Надежда Юрьевна Ефименко Надежда Юрьевна  \n",
      "3   Ермакова Александра Владимировна  Ермакова Але...  \n",
      "4   Терентьева Ольга Владимировна (совмещение 0,5)...  \n",
      "..                                                ...  \n",
      "89  Пашкова Елена Георгиевна Пашкова Елена Георгиевна  \n",
      "90  Третьякова Ирина Владимировна Третьякова Ирина...  \n",
      "91  Хохлов Эдуард Владимирович Хохлов Эдуард Влади...  \n",
      "92  Билькова Вера Алексеевна Билькова Вера Алексеевна  \n",
      "93          Герасев Илья Юрьевич Герасев Илья Юрьевич  \n",
      "\n",
      "[94 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kirill\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "C:\\Users\\Kirill\\AppData\\Local\\Temp/ipykernel_24396/789328408.py:124: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['name'] = names\n",
      "C:\\Users\\Kirill\\AppData\\Local\\Temp/ipykernel_24396/789328408.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['name'] = df['name'].apply(lambda x: x.replace('\\n', ' '))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24396/789328408.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'96461_2019'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m             \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'cool/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.xlsx'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24396/789328408.py\u001b[0m in \u001b[0;36mparse_doc\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    275\u001b[0m                 \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_correct_headers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m                 \u001b[1;31m# убираем лишние ячейки и символы\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m                 \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_cleaner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m                 \u001b[0mparsed_tables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24396/789328408.py\u001b[0m in \u001b[0;36mclean_df\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclean_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_unwanted_symbols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_unwanted_cells\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24396/789328408.py\u001b[0m in \u001b[0;36mremove_unwanted_cells\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# убирает ячейки с нумерацией\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'position'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "# incorrect headers\n",
    "# def detect_and_rename_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     '''!!!!   этот метод для парсинга неправильных заголовков'''\n",
    "    \n",
    "#     cols = df.columns\n",
    "#     res = check_if_columns_ok(cols)\n",
    "\n",
    "#     if not res:        \n",
    "#         for index, cell in enumerate(df[:3].itertuples()):\n",
    "#             res = check_if_columns_ok(cell)\n",
    "#             if res:\n",
    "#                 df.columns = df.iloc[index]\n",
    "#                 df = df[index+1:]\n",
    "#                 break\n",
    "#         if not res:\n",
    "#             return False, df\n",
    "\n",
    "#     cols = df.columns\n",
    "#     renamed_cols = []\n",
    "#     for col in cols:\n",
    "#         renamed_cols.append(rename_col(col))\n",
    "\n",
    "#     df.columns = renamed_cols\n",
    "#     return True, df\n",
    "\n",
    "# TODO: есть еще ситуация - должность и учреждение в одной ячейке. можно решить доп проверкой \n",
    "# - если в должности гбуо, директор, бухгалтер - требует ручного разделения .\n",
    "# TODO: добавить колонку с \"требует ручной работы\" \n",
    "\n",
    "folder = 'data_ids/docx/'\n",
    "\n",
    "\n",
    "class DataCleaner:\n",
    "    \"\"\"убирает лишние данные\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_unwanted_symbols(df):        \n",
    "        df['name'] = df['name'].apply(lambda x: x.replace('\\n', ' '))\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_unwanted_cells(df):\n",
    "        # убирает ячейки с нумерацией\n",
    "        print(df)\n",
    "        df = df[~df['position'].astype(str).str.isdigit()]\n",
    "        return df\n",
    "\n",
    "    def clean_df(self, df):\n",
    "        df = self.remove_unwanted_symbols(df)\n",
    "        df = self.remove_unwanted_cells(df)\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "class CorrectHeadersParser:\n",
    "\n",
    "    '''класс для парсинга таблиц, у которых на месте колонки, которые нам нужны'''\n",
    "\n",
    "    def table_splitter(self, table: pd.DataFrame, file_name) -> list[pd.DataFrame]:\n",
    "        '''разделяет таблицы, в которых учреждение указано внутри таблицы'''\n",
    "\n",
    "        def check_if_same(my_array: list) -> bool:\n",
    "            '''проверяем одинаковые ли колонки'''\n",
    "\n",
    "            first = my_array[0]\n",
    "            for e in my_array[1:]:\n",
    "                if e != first:\n",
    "                    return False\n",
    "            return True\n",
    "\n",
    "        def get_indexes_to_split(table):\n",
    "            index_to_split = []\n",
    "            for e in range(len(table)):\n",
    "                cols = table.iloc[e,:].values\n",
    "                if check_if_same(cols):\n",
    "                    index_to_split.append(e)\n",
    "            return index_to_split\n",
    "\n",
    "\n",
    "        def split_table(table: pd.DataFrame, index_to_split:Union[int, list[int]], file_name) -> list[pd.DataFrame]:\n",
    "            dfs = np.array_split(table, index_to_split)\n",
    "            dfs = [e for e in dfs if len(e) > 0]\n",
    "\n",
    "            result_dfs = []\n",
    "            for df in dfs:\n",
    "                office = df.iloc[0,:][0]\n",
    "                df = df.iloc[1:,:] \n",
    "                df['office'] = office\n",
    "                result_dfs.append(df)\n",
    "            \n",
    "            result_dfs = [e for e in result_dfs if not e.empty]\n",
    "            try:\n",
    "                result_dfs = pd.concat(result_dfs)\n",
    "                return result_dfs\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                print('rogue file---', file_name)\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "        index_to_split = get_indexes_to_split(table)\n",
    "\n",
    "        if not index_to_split:\n",
    "            return table\n",
    "\n",
    "        splitted_dfs = split_table(table, index_to_split, file_name)\n",
    "        return splitted_dfs\n",
    "\n",
    "        \n",
    "    def concat_name(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''соединяем колонки ФИО, если они в разных'''\n",
    "        \n",
    "        if 'name' not in df.columns:\n",
    "            return df\n",
    "        \n",
    "        names_df = df['name']\n",
    "        if isinstance(names_df, str) or isinstance(names_df, pd.Series):\n",
    "            return df  \n",
    "        \n",
    "        names = [' '.join(e) for e in names_df.values]     \n",
    "        \n",
    "        df.drop(columns=['name'], inplace=True)\n",
    "        df['name'] = names\n",
    "        return df\n",
    "\n",
    "\n",
    "    def parse(self, table: pd.DataFrame, file_name) -> pd.DataFrame:\n",
    "        table = self.concat_name(table)\n",
    "        table = self.table_splitter(table, file_name)\n",
    "        return table\n",
    "\n",
    "\n",
    "\n",
    "class DocxParser:\n",
    "\n",
    "    def __init__(self):\n",
    "       self.cols_we_need = ['name','salary', 'position', 'department']\n",
    "       self.parse_correct_headers = CorrectHeadersParser()\n",
    "       self.parse_incorrect_headers = ''\n",
    "       self.all_docs: list[dict[str, Document]]\n",
    "       self.data_cleaner = DataCleaner()\n",
    "\n",
    "    # def read_all_docs(self, path: str)->list[dict[str, Document]]:\n",
    "    #     self.path = path\n",
    "    #     all_docs = []\n",
    "    #     i = 0\n",
    "    #     for doc in os.listdir(path)[:50]:\n",
    "    #         if not doc.endswith('docx'):\n",
    "    #             continue\n",
    "\n",
    "    #         try:\n",
    "    #             all_docs.append({'file':doc,'doc':Document(folder + doc)})\n",
    "    #             i +=1\n",
    "    #         except:\n",
    "    #             print('не вышло--', doc)\n",
    "        \n",
    "    #     self.all_docs = all_docs\n",
    "    #     print(f'нашли и загрузили {i} файлов')\n",
    "\n",
    "        \n",
    "    def read_docx_tables(self, filename, tab_id=None, **kwargs):\n",
    "        \"\"\"\n",
    "        parse table(s) fromt.columnsrd Document (.docx) into Pandas DataFrame(s)\n",
    "\n",
    "        Parameters:\n",
    "            filename:   file name of a Word Document\n",
    "\n",
    "            tab_id:     parse a single table with the index: [tab_id] (counting from 0).\n",
    "                        When [None] - return a list of DataFrames (parse all tables)\n",
    "\n",
    "            kwargs:     arguments to pass to `pd.read_csv()` function\n",
    "\n",
    "        Return: a single DataFrame if tab_id != None or a list of DataFrames otherwise\n",
    "        \"\"\"\n",
    "        def read_docx_tab(tab, **kwargs):\n",
    "            vf = io.StringIO()\n",
    "            writer = csv.writer(vf)\n",
    "            for row in tab.rows:\n",
    "                writer.writerow(cell.text for cell in row.cells)\n",
    "            vf.seek(0)\n",
    "            return pd.read_csv(vf, **kwargs)\n",
    "\n",
    "        doc = Document(filename)\n",
    "        if tab_id is None:\n",
    "            return [read_docx_tab(tab, **kwargs) for tab in doc.tables]\n",
    "        else:\n",
    "            try:\n",
    "                return read_docx_tab(doc.tables[tab_id], **kwargs)\n",
    "            except IndexError:\n",
    "                print('Error: specified [tab_id]: {}  does not exist.'.format(tab_id))\n",
    "                raise\n",
    "            \n",
    "    \n",
    "    @staticmethod\n",
    "    def rename_col(col: str) -> str:\n",
    "        col = col.lower()\n",
    "        if re.search(pattern='(фамилия|имя|фио|ф\\.и\\.о\\.|ф\\.и\\.о|отчество)', string=col):\n",
    "            return \"name\"\n",
    "\n",
    "        elif re.search(pattern='(cреднемесячная|зарпл.|плат[ы, а]|заработн[ой, ая] плат[а, ы]|cреднемесячн[ая, ой]|зарплат[а, ной, ы])', string=col):\n",
    "            return \"salary\"\n",
    "\n",
    "        elif re.search(pattern='(должност[ь, и, ей])', string=col): \n",
    "\n",
    "            return 'position'\n",
    "\n",
    "        elif re.search(pattern='(предприяти[е,я]|учреждени[е,я]|юридическое лицо)', string=col):\n",
    "            return 'department'\n",
    "\n",
    "        return col\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def check_if_columns_ok(cols: tuple) -> bool:\n",
    "        '''проверяем, есть ли в заголовках таблицы название предприятия и другая инфа'''\n",
    "        \n",
    "        cols = list(map(str, cols))\n",
    "        cols = list(map(str.lower, cols))\n",
    "        \n",
    "        ok_cols = 0\n",
    "        company_found = False\n",
    "        for col in cols:\n",
    "            company_pattern = '(предприяти[е,я]|учреждени[е,я]|юридическ[ое,ие])'\n",
    "            res = re.search(pattern=company_pattern, string=col)\n",
    "            if res:\n",
    "                company_found = True\n",
    "                continue\n",
    "            \n",
    "            name_salary_position_pattern = '(фамилия|имя|фио|ф\\.и\\.о\\.|ф\\.и\\.о|отчество|плат[ы, а]|заработная плата|cреднемесячн[ая, ой]|зарплат[а, ной, ы]|должность|)'\n",
    "            \n",
    "            res = re.search(pattern=name_salary_position_pattern, string=col)\n",
    "            if res:\n",
    "                ok_cols+=1\n",
    "\n",
    "        if company_found and ok_cols > 1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def get_all_tables_of_a_doc(self, filename: str) -> list[pd.DataFrame]:\n",
    "\n",
    "        tables = self.read_docx_tables(filename)        \n",
    "        return tables\n",
    "\n",
    "\n",
    "\n",
    "    def parse_doc(self, filename: str) -> pd.DataFrame:\n",
    "        \n",
    "        assert filename.endswith('docx'), 'Формат должен быть .docx!'\n",
    "            \n",
    "        \n",
    "        doc_tables = self.get_all_tables_of_a_doc(filename) \n",
    "        parsed_tables = []\n",
    "        \n",
    "        for table in doc_tables:\n",
    "\n",
    "            # проверяем норм ли заголовки\n",
    "            columns_ok = self.check_if_columns_ok(table)\n",
    "            if not columns_ok:\n",
    "            # если учреждения нет - смотрим параграфы. \n",
    "                # добавить документ в опущенные\n",
    "                pass\n",
    "\n",
    "            else:                \n",
    "                # если заголовки ок, оставляем только нужные\n",
    "\n",
    "                table.columns = [self.rename_col(col) for col in table.columns]\n",
    "                \n",
    "                cols_to_leave = [col for col in table.columns if col in self.cols_we_need]\n",
    "                cols_to_leave = set(cols_to_leave)\n",
    "                table = table[cols_to_leave]\n",
    "                \n",
    "                # проверяем на наличие вложенных таблиц и фио, разнесенных на несколько стаоблцов\n",
    "                table = self.parse_correct_headers.parse(table, filename)\n",
    "                # убираем лишние ячейки и символы\n",
    "                table = self.data_cleaner.clean_df(table)\n",
    "                parsed_tables.append(table)\n",
    "                        \n",
    "        parsed_tables = [e for e in parsed_tables if isinstance(e, pd.DataFrame) and not e.empty]\n",
    "        \n",
    "        if isinstance(parsed_tables, list):\n",
    "            if parsed_tables:\n",
    "                concat_tables = pd.concat(parsed_tables)\n",
    "                return concat_tables\n",
    "        \n",
    "        elif isinstance(parsed_tables, pd.DataFrame):\n",
    "            if not parsed_tables.empty:\n",
    "                return concat_tables\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def parse_folder(self, path: str):\n",
    "        all_tables = []\n",
    "        for doc in os.listdir(path):\n",
    "            pass \n",
    "        #for doc in self.all_docs:\n",
    "        #     tables = self.parse_doc(doc)\n",
    "        #     if tables:\n",
    "        #         # return tables\n",
    "        #         #print(tables)\n",
    "        #         # tables = pd.concat(tables)\n",
    "        #         all_tables.append({'file_name':doc, 'df':tables})\n",
    "        \n",
    "        # return all_tables\n",
    "\n",
    "\n",
    "# file = \"186956_2020_Rukovoditeli_podvedomstvennykh_uchrezhdenii_(kul'tura).docx\"\n",
    "# file = \"102907_2019_Rukovoditeli_podvedomstvennykh_uchrezhdenii_(kul'tura).docx\"\n",
    "# file = '101058_2019_Rukovoditeli_podvedomstvennykh_uchrezhdenii_(obrazovanie).docx'\n",
    "# folder = 'data_ids/docx/'\n",
    "\n",
    "parser = DocxParser()\n",
    "# # res = parser.parse_doc(folder + file)\n",
    "# res = parser.parse_doc(folder + file)\n",
    "\n",
    "# df = res[0]\n",
    "#df\n",
    "\n",
    "for doc in os.listdir(folder):\n",
    "    if not doc.endswith('docx'):\n",
    "        continue\n",
    "    if '96461_2019' in doc:\n",
    "\n",
    "        res = parser.parse_doc(folder + doc)\n",
    "        if isinstance(res, pd.DataFrame):\n",
    "            res.to_excel(folder + 'cool/' + doc + '.xlsx' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cdb02797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        \n",
    "    def read_docx_tables(self, filename, tab_id=None, **kwargs):\n",
    "        \"\"\"\n",
    "        parse table(s) fromt.columnsrd Document (.docx) into Pandas DataFrame(s)\n",
    "\n",
    "        Parameters:\n",
    "            filename:   file name of a Word Document\n",
    "\n",
    "            tab_id:     parse a single table with the index: [tab_id] (counting from 0).\n",
    "                        When [None] - return a list of DataFrames (parse all tables)\n",
    "\n",
    "            kwargs:     arguments to pass to `pd.read_csv()` function\n",
    "\n",
    "        Return: a single DataFrame if tab_id != None or a list of DataFrames otherwise\n",
    "        \"\"\"\n",
    "        def read_docx_tab(tab, **kwargs):\n",
    "            vf = io.StringIO()\n",
    "            writer = csv.writer(vf)\n",
    "            for row in tab.rows:\n",
    "                writer.writerow(cell.text for cell in row.cells)\n",
    "            vf.seek(0)\n",
    "            return pd.read_csv(vf, **kwargs)\n",
    "\n",
    "        doc = Document(filename)\n",
    "        if tab_id is None:\n",
    "            return [read_docx_tab(tab, **kwargs) for tab in doc.tables]\n",
    "        else:\n",
    "            try:\n",
    "                return read_docx_tab(doc.tables[tab_id], **kwargs)\n",
    "            except IndexError:\n",
    "                print('Error: specified [tab_id]: {}  does not exist.'.format(tab_id))\n",
    "                raise\n",
    "            \n",
    "# file = '96461_2019_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii.docx'\n",
    "# tables = read_docx_tables(folder + file)\n",
    "\n",
    "df = tables[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8af8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70bcd165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           department  salary  \\\n",
      "0                                                   1     4\\n   \n",
      "1   ГАПОУ РК «Колледж технологии и предприниматель...  66 582   \n",
      "2   ГАПОУ РК «Колледж технологии и предприниматель...  62 510   \n",
      "3   ГАПОУ РК «Колледж технологии и предприниматель...  46 746   \n",
      "4   ГАПОУ РК «Колледж технологии и предприниматель...  15 489   \n",
      "..                                                ...     ...   \n",
      "89  ГКУ РК «Хозяйственно – эксплуатационная группа...  76 044   \n",
      "90  ГКУ РК «Хозяйственно – эксплуатационная группа...  73 095   \n",
      "91  ГАУ РК «Карельский региональный центр молодежи»\\n  80 692   \n",
      "92  ГАУ РК «Карельский региональный центр молодежи»\\n  61 500   \n",
      "93  ГАУ РК «Карельский региональный центр молодежи»\\n  41 508   \n",
      "\n",
      "                         position                                 position  \\\n",
      "0                               3                                        3   \n",
      "1                     Директор \\n                              Директор \\n   \n",
      "2               Главный бухгалтер                        Главный бухгалтер   \n",
      "3          Заместитель директора                    Заместитель директора    \n",
      "4          Заместитель директора                    Заместитель директора    \n",
      "..                            ...                                      ...   \n",
      "89       Пашкова Елена Георгиевна                                Директор    \n",
      "90  Третьякова Ирина Владимировна                    Заместитель директора   \n",
      "91     Хохлов Эдуард Владимирович                                 Директор   \n",
      "92       Билькова Вера Алексеевна  Заместитель директора- начальник отдела   \n",
      "93           Герасев Илья Юрьевич              Руководитель центра -отдела   \n",
      "\n",
      "                                                 name  \n",
      "0                                                 2 2  \n",
      "1   Германова Галина Николаевна Германова Галина Н...  \n",
      "2   Ефименко Надежда Юрьевна Ефименко Надежда Юрьевна  \n",
      "3   Ермакова Александра Владимировна  Ермакова Але...  \n",
      "4   Терентьева Ольга Владимировна (совмещение 0,5)...  \n",
      "..                                                ...  \n",
      "89  Пашкова Елена Георгиевна Пашкова Елена Георгиевна  \n",
      "90  Третьякова Ирина Владимировна Третьякова Ирина...  \n",
      "91  Хохлов Эдуард Владимирович Хохлов Эдуард Влади...  \n",
      "92  Билькова Вера Алексеевна Билькова Вера Алексеевна  \n",
      "93          Герасев Илья Юрьевич Герасев Илья Юрьевич  \n",
      "\n",
      "[94 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kirill\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "C:\\Users\\Kirill\\AppData\\Local\\Temp/ipykernel_24396/789276579.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['name'] = names\n",
      "C:\\Users\\Kirill\\AppData\\Local\\Temp/ipykernel_24396/789276579.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['name'] = df['name'].apply(lambda x: x.replace('\\n', ' '))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24396/789276579.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'96461_2019'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m             \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'cool/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.xlsx'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24396/789276579.py\u001b[0m in \u001b[0;36mparse_doc\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_correct_headers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                 \u001b[1;31m# убираем лишние ячейки и символы\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                 \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_cleaner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m                 \u001b[0mparsed_tables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24396/789276579.py\u001b[0m in \u001b[0;36mclean_df\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclean_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_unwanted_symbols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_unwanted_cells\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24396/789276579.py\u001b[0m in \u001b[0;36mremove_unwanted_cells\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# убирает ячейки с нумерацией\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'position'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'str'"
     ]
    }
   ],
   "source": [
    "#         for index, cell in enumerate(df[:3].itertuples()):\n",
    "#             res = check_if_columns_ok(cell)\n",
    "#             if res:\n",
    "#                 df.columns = df.iloc[index]\n",
    "#                 df = df[index+1:]\n",
    "#                 break\n",
    "#         if not res:\n",
    "#             return False, df\n",
    "\n",
    "#     cols = df.columns\n",
    "#     renamed_cols = []\n",
    "#     for col in cols:\n",
    "#         renamed_cols.append(rename_col(col))\n",
    "\n",
    "#     df.columns = renamed_cols\n",
    "#     return True, df\n",
    "\n",
    "# TODO: есть еще ситуация - должность и учреждение в одной ячейке. можно решить доп проверкой \n",
    "# - если в должности гбуо, директор, бухгалтер - требует ручного разделения .\n",
    "# TODO: добавить колонку с \"требует ручной работы\" \n",
    "\n",
    "folder = 'data_ids/docx/'\n",
    "\n",
    "\n",
    "class DataCleaner:\n",
    "    \"\"\"убирает лишние данные\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_unwanted_symbols(df):        \n",
    "        df['name'] = df['name'].apply(lambda x: x.replace('\\n', ' '))\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_unwanted_cells(df):\n",
    "        # убирает ячейки с нумерацией\n",
    "        print(df)\n",
    "        df = df[~df['position'].astype(str).str.isdigit()]\n",
    "        return df\n",
    "\n",
    "    def clean_df(self, df):\n",
    "        df = self.remove_unwanted_symbols(df)\n",
    "        df = self.remove_unwanted_cells(df)\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "class CorrectHeadersParser:\n",
    "\n",
    "    '''класс для парсинга таблиц, у которых на месте колонки, которые нам нужны'''\n",
    "\n",
    "    def table_splitter(self, table: pd.DataFrame, file_name) -> list[pd.DataFrame]:\n",
    "        '''разделяет таблицы, в которых учреждение указано внутри таблицы'''\n",
    "\n",
    "        def check_if_same(my_array: list) -> bool:\n",
    "            '''проверяем одинаковые ли колонки'''\n",
    "\n",
    "            first = my_array[0]\n",
    "            for e in my_array[1:]:\n",
    "                if e != first:\n",
    "                    return False\n",
    "            return True\n",
    "\n",
    "        def get_indexes_to_split(table):\n",
    "            index_to_split = []\n",
    "            for e in range(len(table)):\n",
    "                cols = table.iloc[e,:].values\n",
    "                if check_if_same(cols):\n",
    "                    index_to_split.append(e)\n",
    "            return index_to_split\n",
    "\n",
    "\n",
    "        def split_table(table: pd.DataFrame, index_to_split:Union[int, list[int]], file_name) -> list[pd.DataFrame]:\n",
    "            dfs = np.array_split(table, index_to_split)\n",
    "            dfs = [e for e in dfs if len(e) > 0]\n",
    "\n",
    "            result_dfs = []\n",
    "            for df in dfs:\n",
    "                office = df.iloc[0,:][0]\n",
    "                df = df.iloc[1:,:] \n",
    "                df['office'] = office\n",
    "                result_dfs.append(df)\n",
    "            \n",
    "            result_dfs = [e for e in result_dfs if not e.empty]\n",
    "            try:\n",
    "                result_dfs = pd.concat(result_dfs)\n",
    "                return result_dfs\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                print('rogue file---', file_name)\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "        index_to_split = get_indexes_to_split(table)\n",
    "\n",
    "        if not index_to_split:\n",
    "            return table\n",
    "\n",
    "        splitted_dfs = split_table(table, index_to_split, file_name)\n",
    "        return splitted_dfs\n",
    "\n",
    "        \n",
    "    def concat_name(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''соединяем колонки ФИО, если они в разных'''\n",
    "        \n",
    "        if 'name' not in df.columns:\n",
    "            return df\n",
    "        \n",
    "        names_df = df['name']\n",
    "        if isinstance(names_df, str) or isinstance(names_df, pd.Series):\n",
    "            return df  \n",
    "        \n",
    "        names = [' '.join(e) for e in names_df.values]     \n",
    "        \n",
    "        df.drop(columns=['name'], inplace=True)\n",
    "        df['name'] = names\n",
    "        return df\n",
    "\n",
    "\n",
    "    def parse(self, table: pd.DataFrame, file_name) -> pd.DataFrame:\n",
    "        table = self.concat_name(table)\n",
    "        table = self.table_splitter(table, file_name)\n",
    "        return table\n",
    "\n",
    "\n",
    "\n",
    "class DocxParser:\n",
    "\n",
    "    def __init__(self):\n",
    "       self.cols_we_need = ['name','salary', 'position', 'department']\n",
    "       self.parse_correct_headers = CorrectHeadersParser()\n",
    "       self.parse_incorrect_headers = ''\n",
    "       self.all_docs: list[dict[str, Document]]\n",
    "       self.data_cleaner = DataCleaner()\n",
    "\n",
    "    # def read_all_docs(self, path: str)->list[dict[str, Document]]:\n",
    "    #     self.path = path\n",
    "    #     all_docs = []\n",
    "    #     i = 0\n",
    "    #     for doc in os.listdir(path)[:50]:\n",
    "    #         if not doc.endswith('docx'):\n",
    "    #             continue\n",
    "\n",
    "    #         try:\n",
    "    #             all_docs.append({'file':doc,'doc':Document(folder + doc)})\n",
    "    #             i +=1\n",
    "    #         except:\n",
    "    #             print('не вышло--', doc)\n",
    "        \n",
    "    #     self.all_docs = all_docs\n",
    "    #     print(f'нашли и загрузили {i} файлов')\n",
    "\n",
    "        \n",
    "    def read_docx_tables(self, filename, tab_id=None, **kwargs):\n",
    "        \"\"\"\n",
    "        parse table(s) fromt.columnsrd Document (.docx) into Pandas DataFrame(s)\n",
    "\n",
    "        Parameters:\n",
    "            filename:   file name of a Word Document\n",
    "\n",
    "            tab_id:     parse a single table with the index: [tab_id] (counting from 0).\n",
    "                        When [None] - return a list of DataFrames (parse all tables)\n",
    "\n",
    "            kwargs:     arguments to pass to `pd.read_csv()` function\n",
    "\n",
    "        Return: a single DataFrame if tab_id != None or a list of DataFrames otherwise\n",
    "        \"\"\"\n",
    "        def read_docx_tab(tab, **kwargs):\n",
    "            vf = io.StringIO()\n",
    "            writer = csv.writer(vf)\n",
    "            for row in tab.rows:\n",
    "                writer.writerow(cell.text for cell in row.cells)\n",
    "            vf.seek(0)\n",
    "            return pd.read_csv(vf, **kwargs)\n",
    "\n",
    "        doc = Document(filename)\n",
    "        if tab_id is None:\n",
    "            return [read_docx_tab(tab, **kwargs) for tab in doc.tables]\n",
    "        else:\n",
    "            try:\n",
    "                return read_docx_tab(doc.tables[tab_id], **kwargs)\n",
    "            except IndexError:\n",
    "                print('Error: specified [tab_id]: {}  does not exist.'.format(tab_id))\n",
    "                raise\n",
    "            \n",
    "    \n",
    "    @staticmethod\n",
    "    def rename_col(col: str) -> str:\n",
    "        col = col.lower()\n",
    "        if re.search(pattern='(фамилия|имя|фио|ф\\.и\\.о\\.|ф\\.и\\.о|отчество)', string=col):\n",
    "            return \"name\"\n",
    "\n",
    "        elif re.search(pattern='(cреднемесячная|зарпл.|плат[ы, а]|заработн[ой, ая] плат[а, ы]|cреднемесячн[ая, ой]|зарплат[а, ной, ы])', string=col):\n",
    "            return \"salary\"\n",
    "\n",
    "        elif re.search(pattern='(должност[ь, и, ей])', string=col): \n",
    "\n",
    "            return 'position'\n",
    "\n",
    "        elif re.search(pattern='(предприяти[е,я]|учреждени[е,я]|юридическое лицо)', string=col):\n",
    "            return 'department'\n",
    "\n",
    "        return col\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def check_if_columns_ok(cols: tuple) -> bool:\n",
    "        '''проверяем, есть ли в заголовках таблицы название предприятия и другая инфа'''\n",
    "        \n",
    "        cols = list(map(str, cols))\n",
    "        cols = list(map(str.lower, cols))\n",
    "        \n",
    "        ok_cols = 0\n",
    "        company_found = False\n",
    "        for col in cols:\n",
    "            company_pattern = '(предприяти[е,я]|учреждени[е,я]|юридическ[ое,ие])'\n",
    "            res = re.search(pattern=company_pattern, string=col)\n",
    "            if res:\n",
    "                company_found = True\n",
    "                continue\n",
    "            \n",
    "            name_salary_position_pattern = '(фамилия|имя|фио|ф\\.и\\.о\\.|ф\\.и\\.о|отчество|плат[ы, а]|заработная плата|cреднемесячн[ая, ой]|зарплат[а, ной, ы]|должность|)'\n",
    "            \n",
    "            res = re.search(pattern=name_salary_position_pattern, string=col)\n",
    "            if res:\n",
    "                ok_cols+=1\n",
    "\n",
    "        if company_found and ok_cols > 1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def get_all_tables_of_a_doc(self, filename: str) -> list[pd.DataFrame]:\n",
    "\n",
    "        tables = self.read_docx_tables(filename)        \n",
    "        return tables\n",
    "\n",
    "\n",
    "\n",
    "    def parse_doc(self, filename: str) -> pd.DataFrame:\n",
    "        \n",
    "        assert filename.endswith('docx'), 'Формат должен быть .docx!'\n",
    "            \n",
    "        \n",
    "        doc_tables = self.get_all_tables_of_a_doc(filename) \n",
    "        parsed_tables = []\n",
    "        \n",
    "        for table in doc_tables:\n",
    "\n",
    "            # проверяем норм ли заголовки\n",
    "            columns_ok = self.check_if_columns_ok(table)\n",
    "            if not columns_ok:\n",
    "            # если учреждения нет - смотрим параграфы. \n",
    "                # добавить документ в опущенные\n",
    "                pass\n",
    "\n",
    "            else:                \n",
    "                # если заголовки ок, оставляем только нужные\n",
    "\n",
    "                table.columns = [self.rename_col(col) for col in table.columns]\n",
    "                \n",
    "                cols_to_leave = [col for col in table.columns if col in self.cols_we_need]\n",
    "                cols_to_leave = set(cols_to_leave)\n",
    "                table = table[cols_to_leave]\n",
    "                \n",
    "                # проверяем на наличие вложенных таблиц и фио, разнесенных на несколько стаоблцов\n",
    "                table = self.parse_correct_headers.parse(table, filename)\n",
    "                # убираем лишние ячейки и символы\n",
    "                table = self.data_cleaner.clean_df(table)\n",
    "                parsed_tables.append(table)\n",
    "                        \n",
    "        parsed_tables = [e for e in parsed_tables if isinstance(e, pd.DataFrame) and not e.empty]\n",
    "        \n",
    "        if isinstance(parsed_tables, list):\n",
    "            if parsed_tables:\n",
    "                concat_tables = pd.concat(parsed_tables)\n",
    "                return concat_tables\n",
    "        \n",
    "        elif isinstance(parsed_tables, pd.DataFrame):\n",
    "            if not parsed_tables.empty:\n",
    "                return concat_tables\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def parse_folder(self, path: str):\n",
    "        all_tables = []\n",
    "        for doc in os.listdir(path):\n",
    "            pass \n",
    "        #for doc in self.all_docs:\n",
    "        #     tables = self.parse_doc(doc)\n",
    "        #     if tables:\n",
    "        #         # return tables\n",
    "        #         #print(tables)\n",
    "        #         # tables = pd.concat(tables)\n",
    "        #         all_tables.append({'file_name':doc, 'df':tables})\n",
    "        \n",
    "        # return all_tables\n",
    "\n",
    "\n",
    "# file = \"186956_2020_Rukovoditeli_podvedomstvennykh_uchrezhdenii_(kul'tura).docx\"\n",
    "# file = \"102907_2019_Rukovoditeli_podvedomstvennykh_uchrezhdenii_(kul'tura).docx\"\n",
    "# file = '101058_2019_Rukovoditeli_podvedomstvennykh_uchrezhdenii_(obrazovanie).docx'\n",
    "# folder = 'data_ids/docx/'\n",
    "\n",
    "parser = DocxParser()\n",
    "# # res = parser.parse_doc(folder + file)\n",
    "# res = parser.parse_doc(folder + file)\n",
    "\n",
    "# df = res[0]\n",
    "#df\n",
    "\n",
    "for doc in os.listdir(folder):\n",
    "    if not doc.endswith('docx'):\n",
    "        continue\n",
    "    if '96461_2019' in doc:\n",
    "\n",
    "        res = parser.parse_doc(folder + doc)\n",
    "        if isinstance(res, pd.DataFrame):\n",
    "            res.to_excel(folder + 'cool/' + doc + '.xlsx' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a84f61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_if_columns_ok(cols: tuple) -> bool:\n",
    "    '''проверяем, есть ли в заголовках таблицы название предприятия и другая инфа'''\n",
    "    \n",
    "    cols = list(map(str, cols))\n",
    "    cols = list(map(str.lower, cols))\n",
    "    \n",
    "    ok_cols = 0\n",
    "    company_found = False\n",
    "    for col in cols:\n",
    "        company_pattern = '(предприяти[е,я]|учреждени[е,я]|юридическ[ое,ие])'\n",
    "        res = re.search(pattern=company_pattern, string=col)\n",
    "        if res:\n",
    "            company_found = True\n",
    "            continue\n",
    "        \n",
    "        name_salary_position_pattern = '(фамилия|имя|фио|ф.и.о|отчество|заработная плата|cреднемесячная|зарпл|должность|)'\n",
    "        \n",
    "        res = re.search(pattern=name_salary_position_pattern, string=col)\n",
    "        if res:\n",
    "            ok_cols+=1\n",
    "\n",
    "    if company_found and ok_cols > 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "l = ['№ п/п', 'ФИО', 'Должность',\n",
    "       'Среднемесячная заработная плата за 2020 год, руб.']\n",
    "check_if_columns_ok(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "556da855",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def read_docx_tables(filename, tab_id=None, **kwargs):\n",
    "    \"\"\"\n",
    "    parse table(s) fromt.columnsrd Document (.docx) into Pandas DataFrame(s)\n",
    "\n",
    "    Parameters:\n",
    "        filename:   file name of a Word Document\n",
    "\n",
    "        tab_id:     parse a single table with the index: [tab_id] (counting from 0).\n",
    "                    When [None] - return a list of DataFrames (parse all tables)\n",
    "\n",
    "        kwargs:     arguments to pass to `pd.read_csv()` function\n",
    "\n",
    "    Return: a single DataFrame if tab_id != None or a list of DataFrames otherwise\n",
    "    \"\"\"\n",
    "    def read_docx_tab(tab, **kwargs):\n",
    "        vf = io.StringIO()\n",
    "        writer = csv.writer(vf)\n",
    "        for row in tab.rows:\n",
    "            writer.writerow(cell.text for cell in row.cells)\n",
    "        vf.seek(0)\n",
    "        return pd.read_csv(vf, **kwargs)\n",
    "\n",
    "    doc = Document(filename)\n",
    "    if tab_id is None:\n",
    "        return [read_docx_tab(tab, **kwargs) for tab in doc.tables]\n",
    "    else:\n",
    "        try:\n",
    "            return read_docx_tab(doc.tables[tab_id], **kwargs)\n",
    "        except IndexError:\n",
    "            print('Error: specified [tab_id]: {}  does not exist.'.format(tab_id))\n",
    "            raise\n",
    "        \n",
    "file = '96461_2019_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii.docx'\n",
    "file = folder + file\n",
    "tables = read_docx_tables(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3c814969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['detach', 'encoding', 'errors', 'fileno', 'flush', 'getvalue', 'isatty', 'line_buffering', 'newlines', 'read', 'readable', 'readline', 'readlines', 'seek', 'seekable', 'tell', 'truncate', 'writable', 'write', 'writelines']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not StringIO",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24396/3062721128.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mxz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mxz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'*****'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#print(type(b))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not StringIO"
     ]
    }
   ],
   "source": [
    "# file = '96461_2019_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii.docx'\n",
    "# file = folder + file\n",
    "# doc = Document(file)\n",
    "# tab = doc.tables[0]\n",
    "# for row in tab.rows:\n",
    "#     for cell in row.cells:\n",
    "#         print(cell.text)\n",
    "\n",
    "#print(type(b))\n",
    "\n",
    "# vf = io.StringIO()\n",
    "# writer = csv.writer(vf)\n",
    "# for row in tab.rows:\n",
    "#     writer.writerow(cell.text for cell in row.cells)\n",
    "# vf.seek(0)\n",
    "# pd.read_csv(vf, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d1328c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "str_buffer = io.StringIO()\n",
    "bytes_buffer = io.BytesIO()\n",
    "\n",
    "# я могу записывать файл на диск\n",
    "# а могу держать в памяти\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4dcf55-4e5f-439a-b503-f34ce530325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    " объединяет несколько колонок с именем    \n",
    "'''\n",
    "\n",
    "df = pd.read_excel('data_ids/docx/fio.xlsx', engine='openpyxl')\n",
    "\n",
    "def concat_name(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    names_df = df['ФИО']\n",
    "    \n",
    "    if isinstance(names_df, str) or isinstance(names_df, pd.Series):\n",
    "        return df  \n",
    "\n",
    "    names_df = names_df.rename(columns={'ФИО':'old_ФИО'})\n",
    "    names_df['ФИО'] = names_df.apply(' '.join, axis=1)\n",
    "\n",
    "    names_df.drop(columns='old_ФИО', inplace=True)    \n",
    "    df.drop(columns='ФИО', inplace=True)\n",
    "    return pd.concat([df, names_df], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a7af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # такой парсер, который считывал бы заголовок таблицы\n",
    "# # и надпись перед ним\n",
    "\n",
    "# # если в таблицах нет колонки с учерждением смотрим инфу вокруг\n",
    "# # raw = '83336_2017_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii_(ne_ukazany_FIO).docx'\n",
    "# # raw = folder + raw\n",
    "# # # doc = folder + '83350_2017_Rukovoditeli,_zamestiteli_i_glavnye_bukhgaltery_podvedomstvennykh_uchrezhdenii_(ne_ukazany_FIO).docx'\n",
    "# # docx = Document(raw )\n",
    "# res = []\n",
    "\n",
    "# for doc_dict in all_docs:    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# # dfs = read_docx_tables(doc)\n",
    "# #dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883070f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc343f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
